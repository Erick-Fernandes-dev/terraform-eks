# Terraform

## Introdu√ß√£o

O Terraform √© uma ferramenta de infraestrutura como c√≥digo (IaC) desenvolvida pela HashiCorp. Ele permite que voc√™ defina e provisione infraestrutura de forma declarativa usando uma linguagem de configura√ß√£o chamada HashiCorp Configuration Language (HCL).

Principais caracter√≠sticas do Terraform:

- **Multiplataforma:** Suporta diversos provedores de nuvem como AWS, Azure, Google Cloud, al√©m de outras plataformas e servi√ßos
- **Declarativo:** Voc√™ especifica o estado desejado da infraestrutura, e o Terraform determina como alcan√ß√°-lo
- **Gerenciamento de Estado:** Mant√©m registro do estado atual da infraestrutura em arquivos de estado
- **Planejamento de Execu√ß√£o:** Permite visualizar mudan√ßas antes de aplic√°-las atrav√©s do comando 'terraform plan'

Workflow b√°sico do Terraform:

1. **terraform init:** Inicializa um diret√≥rio de trabalho do Terraform
2. **terraform plan:** Cria um plano de execu√ß√£o
3. **terraform apply:** Aplica as mudan√ßas planejadas
4. **terraform destroy:** Remove a infraestrutura provisionada

Benef√≠cios do uso do Terraform:

- **Automatiza√ß√£o:** Reduz erros humanos e aumenta a efici√™ncia
- **Versionamento:** Permite controle de vers√£o da infraestrutura
- **Reusabilidade:** C√≥digo pode ser reutilizado em diferentes projetos
- **Documenta√ß√£o:** O c√≥digo serve como documenta√ß√£o da infraestrutura

O Terraform √© amplamente utilizado em ambientes DevOps e √© uma ferramenta essencial para times que precisam gerenciar infraestrutura em escala.

## Instalando AWS CLI

Segue o link da instala√ß√£o do AWS CLI

https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

Configurando as credenciais do AWS

N√£o √© o ideal usar chave no root, √© necess√°rio criar pelo IAM

**Servi√ßo IMA**

O IAM (Identity and Access Management) √© um servi√ßo da AWS que permite gerenciar de forma segura o acesso aos servi√ßos e recursos da AWS. Principais caracter√≠sticas:

- **Usu√°rios e Grupos:** Permite criar e gerenciar usu√°rios individuais e agrup√°-los
- **Permiss√µes:** Controle granular de acesso atrav√©s de pol√≠ticas (policies)
- **Roles:** Permite que servi√ßos AWS acessem outros servi√ßos de forma segura
- **MFA:** Suporte a autentica√ß√£o multi-fator para maior seguran√ßa

Boas pr√°ticas do IAM:

1. **Criar usu√°rios individuais:** Evite compartilhar credenciais do usu√°rio root
2. **Usar grupos:** Organize usu√°rios em grupos para facilitar o gerenciamento de permiss√µes
3. **Princ√≠pio do menor privil√©gio:** Conceda apenas as permiss√µes necess√°rias
4. **Rota√ß√£o regular de credenciais:** Altere senhas e chaves de acesso periodicamente

Para criar um novo usu√°rio IAM e configurar suas credenciais:

1. Acesse o Console AWS e navegue at√© o servi√ßo IAM
2. Crie um novo usu√°rio com acesso program√°tico
3. Atribua as pol√≠ticas adequadas (diretamente ou via grupos)
4. Salve as credenciais (Access Key ID e Secret Access Key) de forma segura

Criando IAM para permiss√£o de usu√°rio

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%201.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%202.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%203.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%204.png)

Agora vamos configura o AWS CLI

```yaml
aws configure <Access key>
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%205.png)

Execute o seguinte comando

```yaml
aws sts get-caller-identity
```

O comando `aws sts get-caller-identity` √© usado para verificar a identidade atual do usu√°rio autenticado na AWS CLI. Ele retorna tr√™s informa√ß√µes principais:

- **UserId:** O identificador √∫nico do usu√°rio IAM
- **Account:** O n√∫mero da conta AWS que est√° sendo utilizada
- **Arn:** O ARN (Amazon Resource Name) do usu√°rio, que √© um identificador √∫nico para recursos AWS

Este comando √© muito √∫til para:

- Confirmar se as credenciais AWS foram configuradas corretamente
- Verificar qual identidade est√° sendo usada no momento
- Validar as permiss√µes e o acesso √† conta AWS

√â uma boa pr√°tica executar este comando ap√≥s configurar as credenciais AWS para garantir que tudo est√° funcionando como esperado.

Ap√≥s executar o comando

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%206.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%207.png)

Agora √© s√≥ logar na conta com  o IAM criado

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%208.png)

Coloca o nome do IAM criado e depois a senha que voc√™ criou junto com este servi√ßo

## AWS m√£o na massa

## Arquitetura do projeto

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%209.png)

1. Cria√ß√£o da VPC (Virtual Private Cloud)
    1. √â como se fosse o datacenter de rede, parte mais alta, vai ter os recursos do projeto.
    2. A VPC vai ser criada em um regi√£o
2. IGW (Internet Gateway)
3. ALB/NLB (S√£o os Loads Balancers da AWS)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2010.png)

- MNG - Manager Nodes Groups, onde ser√° rodado as VMs

## VPC

VPC (Virtual Private Cloud) √© um servi√ßo fundamental da AWS que permite criar uma rede virtual isolada na nuvem. Principais caracter√≠sticas:

- **Isolamento:** Ambiente de rede totalmente isolado e dedicado na nuvem AWS
- **Controle:** Permite configurar ranges de IP, subnets, tabelas de roteamento e gateways de rede
- **Seguran√ßa:** Oferece m√∫ltiplas camadas de seguran√ßa, incluindo security groups e network ACLs
- **Conectividade:** Pode ser conectada a data centers on-premises atrav√©s de VPN ou Direct Connect

Componentes principais de uma VPC:

1. **Subnets:** Segmentos de rede que podem ser p√∫blicos ou privados
2. **Route Tables:** Controlam o tr√°fego de rede entre as subnets
3. **Internet Gateway:** Permite comunica√ß√£o com a internet
4. **Network ACLs:** Regras de seguran√ßa no n√≠vel da subnet

Uma VPC bem planejada √© essencial para:

- Garantir a seguran√ßa dos recursos na nuvem
- Otimizar a comunica√ß√£o entre servi√ßos
- Controlar custos de tr√°fego de rede
- Facilitar a escalabilidade da infraestrutura

Agora vamos criar uma VPC na AWS

- Pesquise pelo servi√ßo do VPC

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2011.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2012.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2013.png)

VPC criada 

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2014.png)

Agora v√° em actions e edite sua vpc, e marque para Enable DNS hostnames

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2015.png)

O Enable DNS resolution na VPC da AWS serve para:

- **Resolu√ß√£o de DNS interna:** Permite que as inst√¢ncias dentro da VPC resolvam endere√ßos DNS privados e p√∫blicos
- **Nomes de host DNS:** Possibilita que as inst√¢ncias EC2 recebam nomes de host DNS autom√°ticos
- **Integra√ß√£o com Route 53:** Facilita a integra√ß√£o com o servi√ßo de DNS da AWS (Route 53) para resolu√ß√£o de nomes
- **Comunica√ß√£o entre servi√ßos:** Essencial para servi√ßos AWS que dependem de resolu√ß√£o DNS para funcionar corretamente

Esta configura√ß√£o √© fundamental para:

1. Garantir que as inst√¢ncias possam se comunicar usando nomes DNS ao inv√©s de IPs
2. Permitir o funcionamento adequado de servi√ßos AWS que dependem de DNS
3. Facilitar a administra√ß√£o e manuten√ß√£o da infraestrutura

O Enable DNS hostnames na VPC da AWS serve para:

- **Atribui√ß√£o autom√°tica de nomes DNS:** Permite que as inst√¢ncias EC2 recebam automaticamente nomes DNS p√∫blicos quando lan√ßadas em uma subnet p√∫blica
- **Resolu√ß√£o de nomes:** Habilita a capacidade de resolver nomes DNS p√∫blicos para endere√ßos IP p√∫blicos dentro da VPC
- **Conectividade externa:** Facilita a comunica√ß√£o com recursos externos que utilizam nomes DNS em vez de IPs diretos
- **Integra√ß√£o com servi√ßos AWS:** Melhora a integra√ß√£o com outros servi√ßos AWS que dependem de resolu√ß√£o de nomes DNS

Esta configura√ß√£o √© especialmente importante quando:

1. Voc√™ precisa que suas inst√¢ncias sejam acess√≠veis pela internet usando nomes DNS
2. Seus aplicativos requerem resolu√ß√£o de nomes DNS p√∫blicos
3. Voc√™ est√° implementando arquiteturas que dependem de comunica√ß√£o baseada em DNS

## Subnets (privada e p√∫blica)

Subnets s√£o subdivis√µes de uma VPC que permitem organizar e segmentar recursos. Existem dois tipos principais:

### Subnet P√∫blica

Caracter√≠sticas principais:

- **Acesso direto √† Internet:** Possui rota para Internet Gateway (IGW)
- **Recursos expostos:** Ideal para recursos que precisam ser acess√≠veis publicamente
- **Casos de uso:** Load balancers, servidores web, bastions hosts
- **IP p√∫blico:** Inst√¢ncias podem receber endere√ßos IP p√∫blicos automaticamente

### Subnet Privada

Caracter√≠sticas principais:

- **Sem acesso direto √† Internet:** N√£o possui rota direta para IGW
- **Maior seguran√ßa:** Recursos protegidos do acesso p√∫blico direto
- **Acesso √† Internet via NAT:** Usa NAT Gateway para acesso sainte √† Internet
- **Casos de uso:** Bancos de dados, servi√ßos internos, backends de aplica√ß√µes

Boas pr√°ticas de configura√ß√£o:

1. **Segmenta√ß√£o:** Distribuir recursos entre subnets p√∫blicas e privadas conforme necessidade de acesso
2. **Alta disponibilidade:** Criar subnets em m√∫ltiplas Availability Zones
3. **CIDR adequado:** Planejar ranges de IP com espa√ßo para crescimento
4. **Documenta√ß√£o:** Manter registro claro da organiza√ß√£o e prop√≥sito de cada subnet

Criando uma subnet

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2016.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2017.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2018.png)

Subnets criadas

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2019.png)

## Tags na subnets para AWS Load Balancer Controller

### Requisitos de sub-redes para n√≥s

√â poss√≠vel implantar n√≥s e recursos do Kubernetes nas mesmas
 sub-redes que voc√™ especifica ao criar o cluster. Por√©m, isso n√£o √© 
necess√°rio. Isso porque tamb√©m √© poss√≠vel implantar n√≥s e recursos do 
Kubernetes em sub-redes que voc√™ n√£o especificou quando criou o cluster.
 Se voc√™ implantar n√≥s em sub-redes diferentes, o Amazon EKS n√£o criar√° 
interfaces de rede de cluster nelas. Qualquer sub-rede na qual voc√™ 
implante n√≥s e recursos do Kubernetes deve atender aos seguintes 
requisitos:

- As sub-redes devem ter endere√ßos IP dispon√≠veis suficientes para implantar todos os n√≥s e recursos do Kubernetes.
- Se quiser que o Kubernetes atribua endere√ßos `IPv6` a Pods e servi√ßos, voc√™ dever√° ter um bloco CIDR `IPv6` e um bloco CIDR `IPv4` associados √† sub-rede. Para obter mais informa√ß√µes, consulte [Associar um bloco CIDR IPv6 √† sua sub-rede](https://docs.aws.amazon.com/vpc/latest/userguide/working-with-subnets.html#subnet-associate-ipv6-cidr) no Guia do usu√°rio do Amazon VPC. As tabelas de rotas associadas √†s sub-redes devem incluir rotas para endere√ßos `IPv4` e `IPv6`. Para obter mais informa√ß√µes, consulte [Rotas](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html#route-table-routes), no Guia do usu√°rio da Amazon VPC. Pods recebem apenas um endere√ßo `IPv6`. Por√©m, as interfaces de rede que criadas pelo Amazon EKS para o seu cluster e os seus n√≥s recebem um endere√ßo `IPv4` e um endere√ßo `IPv6`.
- Se precisar de acesso de entrada pela Internet aos
Pods, certifique-se de ter pelo menos uma sub-rede p√∫blica com endere√ßos IP dispon√≠veis suficientes para implantar balanceadores de carga e
ingressos. Voc√™ pode implantar balanceadores de carga em sub-redes
p√∫blicas. Balanceadores de carga podem balancear carga para Pods em
sub-redes privadas ou p√∫blicas. Conv√©m implantar n√≥s em sub-redes
privadas, se poss√≠vel.
- Se voc√™ planeja implantar n√≥s em uma sub-rede p√∫blica, esta dever√° atribuir automaticamente endere√ßos p√∫blicos `IPv4` ou endere√ßos `IPv6`. Se voc√™ implantar n√≥s em uma sub-rede privada que tenha um bloco CIDR `IPv6` associado, esta tamb√©m dever√° atribuir endere√ßos `IPv6` automaticamente. Se voc√™ usou um modelo [Criar uma Amazon VPC para o cluster do Amazon EKS](https://docs.aws.amazon.com/pt_br/eks/latest/userguide/creating-a-vpc.html)Amazon EKS AWS CloudFormation para implantar sua VPC ap√≥s 26 de mar√ßo de 2020, essa configura√ß√£o est√° ativada. Se voc√™ utilizou os modelos para
implantar sua VPC antes dessa data ou se utilizar sua pr√≥pria VPC,
dever√° habilitar essa configura√ß√£o manualmente. Para obter mais
informa√ß√µes, consulte [Modificar o atributo de endere√ßamento IPv4 p√∫blico para sua sub-rede e](https://docs.aws.amazon.com/vpc/latest/userguide/working-with-subnets.html#subnet-public-ip) [Modificar o atributo de endere√ßamento IPv6 para sua sub-rede](https://docs.aws.amazon.com/vpc/latest/userguide/working-with-subnets.html#subnet-ipv6) no [Guia do usu√°rio do Amazon VPC](https://docs.aws.amazon.com/vpc/latest/userguide/).
- Se a sub-rede na qual voc√™ implanta um n√≥ for privada e sua tabela de rotas n√£o incluir uma rota para um [dispositivo de convers√£o de endere√ßos de rede (NAT)](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html) (`IPv4`) ou um [gateway somente de sa√≠da](https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html) (`IPv6`), adicione endpoints de VPC usando o AWS PrivateLink √† sua VPC. Os
endpoints de VPC s√£o necess√°rios para todos os servi√ßos AWS com os quais seus n√≥s e Pods precisam se comunicar. Os exemplos incluem o Amazon
ECR, o Elastic Load Balancing, o Amazon CloudWatch, o AWS Security Token Service e o Amazon Simple Storage Service (Amazon S3). O endpoint deve
incluir a sub-rede na qual os n√≥s se encontram. Nem todos os servi√ßos do AWS oferecem suporte a endpoints de VPC. Para obter mais informa√ß√µes,
consulte [O que √© o AWS PrivateLink?](https://docs.aws.amazon.com/vpc/latest/privatelink/what-is-privatelink.html) e [AWS servi√ßos que se integram ao AWS PrivateLink](https://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.html). Para obter uma lista de mais requisitos para o Amazon EKS, consulte [Implementar clusters privados com acesso limitado √† internet](https://docs.aws.amazon.com/pt_br/eks/latest/userguide/private-clusters.html).
- Se quiser implantar balanceadores de carga em uma sub-rede, esta deve ter a seguinte etiqueta:

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2020.png)

Vamos colocar  tags na subnets  privadas

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2021.png)

V√° nesse link https://docs.aws.amazon.com/pt_br/eks/latest/userguide/network-reqs.html

copie e cole isso aqui [`kubernetes.io/role/internal-elb`](http://kubernetes.io/role/internal-elb) valor 1

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2022.png)

Depois de criado, o nosso controller vai entender que essa √© uma subnet privada, que serve para criar Load Balancer que s√£o privados

Agora vamos padronizar as subnets p√∫blicas:

Copie e cole, [`kubernetes.io/role/elb`](http://kubernetes.io/role/elb) valor 1

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2023.png)

## Internet Gateway e Route Table

### O que √© uma Internet Gateway üíª

Um Internet Gateway √© um componente de rede que permite a comunica√ß√£o entre uma rede privada (como uma Virtual Private Cloud - VPC) e a internet p√∫blica. Ele atua como um ponto de entrada e sa√≠da para o tr√°fego de dados, permitindo que inst√¢ncias dentro da rede privada se conectem √† internet e que usu√°rios da internet acessem recursos dentro da rede privada, se configurado para isso.

Os principais recursos de um Internet Gateway incluem:

1. **Conex√£o com a Internet**: Permite que as inst√¢ncias em uma rede privada acessem a internet para enviar e receber dados.
2. **Endere√ßamento IP**: Facilita a atribui√ß√£o de endere√ßos IP p√∫blicos √†s inst√¢ncias, permitindo que elas sejam acess√≠veis a partir da internet.
3. **Roteamento**: Trabalha em conjunto com tabelas de roteamento para direcionar o tr√°fego de entrada e sa√≠da.
4. **Escalabilidade**: Pode lidar com grandes volumes de tr√°fego, permitindo que a rede se expanda conforme necess√°rio.

Em ambientes de nuvem, como o Amazon Web Services (AWS), o Internet Gateway √© um recurso essencial para permitir que as inst√¢ncias em uma VPC se comuniquem com a internet.

### O que √© uma Route Table üíª

Uma Route Table (tabela de roteamento) √© um componente fundamental em redes de computadores que define como o tr√°fego de rede deve ser direcionado. Ela cont√©m uma lista de rotas, que s√£o regras que informam ao roteador ou √† inst√¢ncia de rede como encaminhar pacotes de dados para diferentes destinos.

Aqui est√£o alguns pontos importantes sobre as Route Tables:

1. **Entradas de Rota**: Cada entrada na tabela de roteamento especifica um destino (geralmente um endere√ßo IP ou uma rede) e o pr√≥ximo salto (next hop) para onde o tr√°fego deve ser enviado. Isso pode incluir endere√ßos IP de outros roteadores ou gateways.
2. **Direcionamento de Tr√°fego**: Quando um pacote de dados √© enviado, o dispositivo de rede consulta a tabela de roteamento para determinar a melhor rota para o destino. Se houver uma correspond√™ncia na tabela, o pacote √© enviado para o pr√≥ximo salto especificado.
3. **Tipos de Rotas**: As rotas podem ser est√°ticas (definidas manualmente) ou din√¢micas (aprendidas automaticamente atrav√©s de protocolos de roteamento).
4. **Roteamento em Nuvem**: Em ambientes de nuvem, como o Amazon Web Services (AWS), as Route Tables s√£o usadas para gerenciar o tr√°fego entre sub-redes, Internet Gateways e outros recursos de rede. Cada sub-rede em uma VPC (Virtual Private Cloud) pode ser associada a uma Route Table espec√≠fica.
5. **Prioridade**: As entradas na tabela de roteamento podem ter diferentes prioridades, e o roteador geralmente utiliza a rota mais espec√≠fica (com a m√°scara de sub-rede mais longa) quando decide como encaminhar o tr√°fego.

Em resumo, a Route Table √© essencial para garantir que os dados sejam enviados de forma eficiente e correta atrav√©s de uma rede, seja ela local ou na nuvem.

### Criando um Gateway

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2024.png)

Agora vamos fazer um atach no VPC

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2025.png)

Selecionamos o VPC criado

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2026.png)

Agora vamos criar uma Route Table

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2027.png)

Selecione sua VPC ao criar uma route table

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2028.png)

Agora vamos adicionar uma rota

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2029.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2030.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2031.png)

Agora vamos editar o route table de uma subnet p√∫blica

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2032.png)

Selecione o route table que foi criado

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2033.png)

A partir desse momento as nossas sub-nets p√∫blicas est√£o com rota externa.

## Auto Assign Public Ip

O Auto Assign Public IP √© uma configura√ß√£o em redes de nuvem que permite a atribui√ß√£o autom√°tica de endere√ßos IP p√∫blicos para inst√¢ncias quando elas s√£o lan√ßadas em uma sub-rede espec√≠fica.

Principais aspectos do Auto Assign Public IP:

- **Atribui√ß√£o Autom√°tica:** Quando habilitado, cada nova inst√¢ncia criada na sub-rede receber√° automaticamente um endere√ßo IP p√∫blico al√©m do seu IP privado
- **Configura√ß√£o por Sub-rede:** Esta configura√ß√£o pode ser habilitada ou desabilitada no n√≠vel da sub-rede, permitindo controle granular sobre quais partes da sua rede podem ter IPs p√∫blicos
- **Seguran√ßa:** √ötil para inst√¢ncias que precisam de acesso direto √† internet, mas deve ser usado com cautela e apenas em sub-redes p√∫blicas
- **Flexibilidade:** Mesmo com esta configura√ß√£o habilitada, voc√™ ainda pode sobrescrever a configura√ß√£o padr√£o ao lan√ßar inst√¢ncias individuais

Esta configura√ß√£o √© particularmente importante em arquiteturas que requerem acesso √† internet, como servidores web em sub-redes p√∫blicas.

- Configurando o subnet para auto assign public ip

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2034.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2035.png)

## Bastion Host para confirmar

O Amazon EC2 (Elastic Compute Cloud) √© um servi√ßo web que fornece capacidade computacional redimension√°vel na nuvem da AWS. Em termos simples, √© um servi√ßo que permite alugar computadores virtuais na nuvem.

Principais caracter√≠sticas do EC2:

- **Elasticidade:** Capacidade de aumentar ou diminuir recursos computacionais conforme necess√°rio
- **Flexibilidade:** Diversos tipos de inst√¢ncias otimizadas para diferentes casos de uso
- **Integra√ß√£o:** Funciona perfeitamente com outros servi√ßos AWS
- **Controle total:** Voc√™ tem controle completo sobre suas inst√¢ncias virtuais
- **Seguran√ßa:** Integra√ß√£o com Amazon VPC e diversos recursos de seguran√ßa

O EC2 √© fundamental para hospedar aplica√ß√µes, executar servidores, e realizar processamento de dados na nuvem AWS.

### Criar um par de chaves

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2036.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2037.png)

### Agora vamos criar uma instancia na AWS

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2038.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2039.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2040.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2041.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2042.png)

Entrando no modo SSH da instancia

```bash
chmod 600 /Downloads/comunidadedevops-keypar.pem

ssh -i ~/Downloads/comunidadedevops-keypar.pem ubuntu@18.207.102.230
```

A flag -i √© para especificar que vamos utilizar chaves privadas

Depois execute um ping para ver se tudo est√° funcionando certo

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2043.png)

Agora √© s√≥ terminar a m√°quina

## NAT Gateway e Route Table

Um NAT Gateway (Network Address Translation Gateway) √© um servi√ßo gerenciado que permite que inst√¢ncias em uma sub-rede privada se comuniquem com a internet ou outros servi√ßos AWS, mantendo essas inst√¢ncias inacess√≠veis a partir da internet.

Principais caracter√≠sticas do NAT Gateway:

1. **Acesso √† Internet para Sub-redes Privadas:** Permite que inst√¢ncias em sub-redes privadas acessem a internet para atualiza√ß√µes, downloads e outros servi√ßos, sem expor essas inst√¢ncias diretamente √† internet.
2. **Alta Disponibilidade:** √â automaticamente redundante dentro de uma zona de disponibilidade e pode ser implantado em m√∫ltiplas zonas para maior resili√™ncia.
3. **Escalabilidade Autom√°tica:** Pode lidar com at√© 45 Gbps de largura de banda e escala automaticamente conforme necess√°rio.
4. **Seguran√ßa Aprimorada:** Fornece uma camada adicional de seguran√ßa ao ocultar inst√¢ncias privadas da internet, permitindo apenas tr√°fego de sa√≠da.
5. **Gerenciamento Simplificado:** Como servi√ßo gerenciado pela AWS, n√£o requer administra√ß√£o ou manuten√ß√£o por parte do usu√°rio.

O NAT Gateway √© essencial em arquiteturas que seguem as melhores pr√°ticas de seguran√ßa, onde recursos que n√£o precisam ser acess√≠veis publicamente s√£o colocados em sub-redes privadas, mas ainda precisam de acesso √† internet para funcionalidades espec√≠ficas.

### Vamos criar um NAT

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2044.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2045.png)

Com o NAT criado, agora vamos criar um Route Table para as rotas privadas

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2046.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2047.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2048.png)

Com as NAT adicionadas as Route Tables, agora vamos implementar as subnets nas Route Tables

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2049.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2050.png)

## Cluster IAM role

O Cluster IAM Role (Identity and Access Management Role) √© uma fun√ß√£o que define as permiss√µes que um cluster na AWS pode ter para interagir com outros servi√ßos AWS. √â um componente crucial para garantir a seguran√ßa e o controle de acesso adequado.

Principais aspectos do Cluster IAM Role:

- **Controle de Acesso:** Define quais servi√ßos AWS o cluster pode acessar e quais a√ß√µes pode executar
- **Seguran√ßa:** Implementa o princ√≠pio do menor privil√©gio, garantindo que o cluster tenha apenas as permiss√µes necess√°rias
- **Gerenciamento Centralizado:** Permite administrar permiss√µes de forma centralizada atrav√©s de pol√≠ticas IAM
- **Flexibilidade:** Pode ser modificado conforme as necessidades do cluster mudam, sem necessidade de altera√ß√µes no c√≥digo

O Cluster IAM Role √© especialmente importante em ambientes que utilizam servi√ßos como EKS (Elastic Kubernetes Service), ECS (Elastic Container Service) ou outros servi√ßos que necessitam interagir com m√∫ltiplos recursos AWS.

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2051.png)

Vamos criar uma role

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2052.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2053.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2054.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2055.png)

## Cria√ß√£o do cluster EKS

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2056.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2057.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2058.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2059.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2060.png)

D√° um next no restante e clique em Create

Aguarde at√© o cluster ser criado

Executando requisi√ß√£o de Host

```bash
host F9FB1BFE3D622507E1C3D476C63AAE87.gr7.us-east-1.eks.amazonaws.com
```

Execute o seguinte para estabelecer uma conex√£o TCP com o endpoint especificado na porta 443, enquanto fornece informa√ß√µes detalhadas sobre o processo.

```bash
nc -v F9FB1BFE3D622507E1C3D476C63AAE87.gr7.us-east-1.eks.amazonaws.com 443
```

- `nc`: √â o comando para o netcat, uma ferramenta de rede que pode ser usada para criar conex√µes TCP ou UDP.
- `v`: Esta op√ß√£o ativa o modo "verbose", que fornece informa√ß√µes detalhadas
sobre o que o netcat est√° fazendo, como a conex√£o sendo estabelecida.
- [`F9FB1BFE3D622507E1C3D476C63AAE87.gr7.us-east-1.eks.amazonaws.com](http://f9fb1bfe3d622507e1c3d476c63aae87.gr7.us-east-1.eks.amazonaws.com/):` Este √© o endere√ßo do host ao qual voc√™ est√° tentando se conectar.
Parece ser um endpoint de um servi√ßo na AWS (Amazon Web Services),
especificamente no Elastic Kubernetes Service (EKS).
- `443`: Este √© o n√∫mero da porta √† qual voc√™ est√° se conectando. A porta 443 √© comumente usada para conex√µes HTTPS, que s√£o seguras.

Que vai retornar o seguinte:

```bash
Connection to F9FB1BFE3D622507E1C3D476C63AAE87.gr7.us-east-1.eks.amazonaws.com (34.192.201.25) 443 port [tcp/https] succeeded!
```

Comando para pegar o kubeconfig do cluster criado no servi√ßo do EKS

```bash
aws eks update-kubeconfig --region us-east-1 --name comunidadedevops-eks 
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2061.png)

```bash
kubectl config get-contexts
```

Comando para come√ßar autilizar o cluster EKS criado

```bash
kubectl config use-context arn:aws:eks:us-east-1:654654576685:cluster/comunidadedevops-eks  
```

```bash
kubectl get po -A
```

[https://www.notion.so](https://www.notion.so)

## Node IAM Role

**Node IAM Role** √© um conceito no contexto do **AWS Identity and Access Management (IAM)**, que permite atribuir permiss√µes espec√≠ficas a inst√¢ncias de EC2 ou n√≥s (nodes) dentro de um cluster Kubernetes que est√£o rodando na AWS. Esse papel √© essencial para controlar quais a√ß√µes esses n√≥s podem realizar nos servi√ßos da AWS.

### Contexto

No caso de clusters Kubernetes gerenciados pela AWS, como o **Amazon EKS (Elastic Kubernetes Service)**, os n√≥s do cluster (geralmente inst√¢ncias EC2) frequentemente precisam interagir com outros servi√ßos da AWS, como S3, DynamoDB, CloudWatch, ou outros. Para permitir isso de forma segura e controlada, utilizamos **IAM Roles**.

---

### Como funciona uma Node IAM Role?

1. **Cria√ß√£o da Role**:
    - Uma **IAM Role** √© criada e configurada com permiss√µes espec√≠ficas usando **IAM Policies**.
    - Exemplo: uma policy que permite acesso de leitura/grava√ß√£o em um bucket S3.
2. **Associa√ß√£o com as inst√¢ncias (n√≥s)**:
    - Essa role √© associada ao perfil de inst√¢ncia (Instance Profile) da inst√¢ncia EC2 que ser√° usada como n√≥ no cluster.
3. **Execu√ß√£o de opera√ß√µes**:
    - Quando a inst√¢ncia tenta acessar recursos da AWS (por exemplo, fazer upload para um bucket S3), a Node IAM Role fornece as credenciais tempor√°rias necess√°rias para autentica√ß√£o e autoriza√ß√£o.
4. **Renova√ß√£o autom√°tica**:
    - Credenciais tempor√°rias associadas √† role s√£o geradas automaticamente e renovadas periodicamente, eliminando a necessidade de armazenar chaves de acesso est√°ticas.

---

### Benef√≠cios do uso de Node IAM Role:

- **Seguran√ßa**:
    - Evita o uso de credenciais fixas ou hard-coded nos n√≥s.
    - As permiss√µes podem ser limitadas com o princ√≠pio do **menor privil√©gio**.
- **Gerenciamento centralizado**:
    - As permiss√µes s√£o gerenciadas por meio de pol√≠ticas IAM, centralizando o controle de acesso.
- **Escalabilidade**:
    - Uma mesma role pode ser usada por v√°rias inst√¢ncias/n√≥s, simplificando a configura√ß√£o.

---

### Exemplo de Configura√ß√£o:

### 1. Criar a IAM Role com permiss√µes:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::meu-bucket/*"
        }
    ]
}

```

### 2. Associar a Role ao Instance Profile:

Ao lan√ßar a inst√¢ncia EC2 (ou gerenciar os n√≥s via o EKS), associe o **Instance Profile** com a role criada.

---

### No contexto de Kubernetes e Amazon EKS:

Dentro de um cluster Kubernetes, o uso do plugin **Amazon EKS Pod Identity (IAM Roles for Service Accounts)** permite atribuir roles diretamente aos **pods**, garantindo permiss√µes mais granulares por pod, enquanto as **Node IAM Roles** fornecem permiss√µes gerais para os n√≥s como um todo.

### Conclus√£o:

A **Node IAM Role** √© fundamental para garantir que os n√≥s (ou inst√¢ncias EC2) possam acessar os servi√ßos da AWS de maneira segura e eficiente, sem o uso de credenciais fixas, seguindo as melhores pr√°ticas de seguran√ßa.

### Criando um Node Group

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2062.png)

Vamos criar um role

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2063.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2064.png)

Agora ir√° colocar as seguintes permiss√µes:

- [AmazonEKS_CNI_Policy](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/policies/details/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonEKS_CNI_Policy)
- [AmazonEKSWorkerNodePolicy](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/policies/details/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonEKSWorkerNodePolicy)
- [AmazonEC2ContainerRegistryReadOnly](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/policies/details/arn%3Aaws%3Aiam%3A%3Aaws%3Apolicy%2FAmazonEC2ContainerRegistryReadOnly)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2065.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2066.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2067.png)

## Managed Node Group

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2068.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2069.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2070.png)

Quero que rode nas subnets privadas

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2071.png)

Permitir acesso pelo par de chaves criadas

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2072.png)

Depois de criado, perceba que ele ir√° criar uma inst√¢ncia no EC2  com todas as configura√ß√µes do kubernetes

Antes quando executava o `kubectl get nodes` retornava ***No resources found***

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2073.png)

Fonte da imagem: Retirada do curso

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2074.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2075.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2076.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2077.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2078.png)

## OIDC Provider

O **OIDC Provider** no contexto da AWS √© um servi√ßo que permite a integra√ß√£o de identidades externas usando o padr√£o **OpenID Connect (OIDC)** para autentica√ß√£o e autoriza√ß√£o. Ele √© frequentemente utilizado em configura√ß√µes de servi√ßos como o **Amazon EKS** para permitir que aplica√ß√µes ou workloads autenticadas por identidades externas acessem recursos da AWS de maneira segura e granular.

---

### **O que √© OIDC?**

**OpenID Connect (OIDC)** √© um protocolo de autentica√ß√£o baseado em OAuth 2.0. Ele √© amplamente usado para autenticar usu√°rios e servi√ßos em sistemas distribu√≠dos. No contexto da AWS, o **OIDC Provider** atua como um intermedi√°rio confi√°vel que valida identidades externas.

---

### **OIDC Provider na AWS**

### **Defini√ß√£o**

Um **OIDC Provider** na AWS √© uma entidade configurada para aceitar tokens emitidos por um provedor de identidade externo compat√≠vel com OIDC, como o **Google**, **Auth0**, **Keycloak** ou at√© mesmo um servi√ßo interno que implemente OIDC. No caso de clusters Kubernetes, o pr√≥prio **EKS Cluster** pode atuar como um provedor de identidade OIDC.

### **Fun√ß√£o**

- Permitir que identidades autenticadas por um provedor externo obtenham acesso aos recursos da AWS usando permiss√µes associadas a **IAM Roles**.
- Substituir a necessidade de usar credenciais fixas ou chaves de acesso em workloads.

---

### **Como funciona o OIDC Provider no AWS EKS?**

1. **Configura√ß√£o do OIDC Provider**:
    - Durante a cria√ß√£o de um cluster EKS, a AWS cria ou permite configurar um **OIDC Provider** espec√≠fico para o cluster.
    - Esse provider emite **tokens de identidade JWT** que podem ser usados pelos workloads (como pods) para se autenticarem com servi√ßos AWS.
2. **IAM Roles for Service Accounts (IRSA)**:
    - O **OIDC Provider** √© usado para associar permiss√µes do IAM diretamente a service accounts no Kubernetes.
    - Com isso, os pods podem herdar permiss√µes espec√≠ficas sem depender de permiss√µes atribu√≠das ao n√≥ onde est√£o executando.
3. **Fluxo de funcionamento**:
    - Um pod assume uma **service account** que est√° vinculada a uma IAM Role via OIDC.
    - O OIDC Provider valida o token do pod emitido pelo Kubernetes.
    - Com o token validado, a AWS fornece credenciais tempor√°rias para o pod acessar os servi√ßos AWS definidos na pol√≠tica da role.

---

### **Benef√≠cios do OIDC Provider**

- **Seguran√ßa melhorada**:
    - Elimina a necessidade de usar credenciais fixas nos workloads.
    - Permite autentica√ß√£o granular baseada em tokens tempor√°rios.
- **Gerenciamento simplificado**:
    - Usa associa√ß√µes entre IAM Roles e service accounts para fornecer permiss√µes espec√≠ficas a pods ou workloads.
- **Flexibilidade**:
    - Suporte a m√∫ltiplos provedores de identidade OIDC externos.

---

### **Exemplo de Uso no EKS**

### Passo 1: Configurar o OIDC Provider

1. Verifique o **OIDC URL** associado ao cluster EKS.
2. Vincule o OIDC Provider com o IAM:
    
    ```bash
    aws eks describe-cluster --name <cluster-name> --query "cluster.identity.oidc.issuer" --output text
    
    ```
    
3. Crie um **OIDC Provider** na AWS:
    
    ```bash
    aws iam create-open-id-connect-provider \
        --url <OIDC-issuer-URL> \
        --thumbprint-list <certificate-thumbprint> \
        --client-id-list sts.amazonaws.com
    
    ```
    

### Passo 2: Associar uma Role ao Pod

- Configure uma **IAM Role** vinculada a uma service account no Kubernetes.
- Use o token emitido pelo OIDC Provider para autenticar o pod e permitir acesso aos recursos da AWS.

---

### Conclus√£o

O **OIDC Provider** no contexto da AWS √© uma ferramenta poderosa para autentica√ß√£o baseada em identidades externas. Ele √© essencial em configura√ß√µes modernas, como o **Amazon EKS**, onde garante permiss√µes seguras e din√¢micas para workloads Kubernetes, simplificando o gerenciamento e melhorando a seguran√ßa do sistema.

### Execute o seguinte comando para criar um provedor OIDC do IAM para o cluster.

```bash
oidc_id=$(aws eks describe-cluster --name comunidadedevops-eks --query "cluster.identity.oidc.issuer" --output text | cut -d '/' -f 5)
```

1. Determine se um provedor do OIDC do IAM com seu ID do emissor do cluster j√° est√° em sua conta.

```bash
aws iam list-open-id-connect-providers | grep $oidc_id | cut -d "/" -f4
```

Se um resultado for retornado, significar√° que voc√™
 j√° tem um provedor OIDC do IAM para o cluster e poder√° pular a pr√≥xima 
etapa. Se nenhum resultado for retornado, voc√™ dever√° criar um provedor 
OIDC do IAM para seu cluster.

1. Crie o provedor de identidade de OIDC do IAM para o cluster com o comando a seguir.

```bash
eksctl utils associate-iam-oidc-provider --cluster $cluster_name --approve
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2079.png)

## DEPLOY AWS Load Balancer Controller

Um **Load Balancer** (ou balanceador de carga) √© um componente de infraestrutura usado para distribuir tr√°fego de rede ou de aplica√ß√£o entre m√∫ltiplos servidores ou inst√¢ncias, garantindo alta disponibilidade, escalabilidade e desempenho para sistemas e aplica√ß√µes. Ele atua como um ponto de entrada centralizado, redirecionando as requisi√ß√µes dos clientes para os servidores dispon√≠veis de forma eficiente.

---

### **Principais Fun√ß√µes de um Load Balancer**

1. **Distribui√ß√£o de Carga**:
    - Garante que o tr√°fego seja distribu√≠do uniformemente entre os servidores para evitar sobrecarga em um √∫nico ponto.
2. **Alta Disponibilidade**:
    - Caso um servidor falhe, o Load Balancer redireciona automaticamente o tr√°fego para outros servidores dispon√≠veis.
3. **Escalabilidade**:
    - Facilita a adi√ß√£o ou remo√ß√£o de servidores de forma transparente para os usu√°rios.
4. **Desempenho**:
    - Otimiza o tempo de resposta ao direcionar o tr√°fego para servidores com menor carga ou menor lat√™ncia.
5. **Seguran√ßa**:
    - Pode incluir recursos como Termina√ß√£o de SSL/TLS (HTTPS) e prote√ß√£o contra ataques DDoS.

---

### **Tipos de Load Balancer**

### 1. **Baseado em Camada de Rede (Layer 4 - TCP/UDP)**

- Opera na camada de transporte do modelo OSI.
- Distribui tr√°fego com base em informa√ß√µes de rede, como IP de origem/destino e porta.
- Exemplo: **AWS Network Load Balancer (NLB)**.

### 2. **Baseado em Camada de Aplica√ß√£o (Layer 7 - HTTP/HTTPS)**

- Opera na camada de aplica√ß√£o do modelo OSI.
- Pode inspecionar o conte√∫do das requisi√ß√µes, como URL, cabe√ßalhos e cookies, para tomar decis√µes sobre o roteamento.
- Exemplo: **AWS Application Load Balancer (ALB)**.

### 3. **Global Load Balancer (DNS-based)**

- Distribui tr√°fego entre diferentes regi√µes ou data centers.
- Exemplo: **AWS Route 53** (com DNS failover).

---

### **Load Balancer na Nuvem**

Na nuvem, servi√ßos gerenciados de Load Balancer s√£o amplamente utilizados. Eles eliminam a necessidade de gerenciar infraestrutura f√≠sica e oferecem integra√ß√£o nativa com outros servi√ßos.

### Exemplos:

- **AWS Elastic Load Balancer (ELB)**:
    - Suporte para Application Load Balancer, Network Load Balancer e Gateway Load Balancer.
- **Google Cloud Load Balancing**.
- **Azure Load Balancer**.

---

### **Como o Load Balancer Funciona?**

1. **Cliente envia uma requisi√ß√£o**:
    - O cliente acessa o endpoint do Load Balancer.
2. **Load Balancer distribui o tr√°fego**:
    - Ele escolhe um servidor backend com base em pol√≠ticas configuradas, como Round Robin, Least Connections, ou IP Hash.
3. **Servidor backend processa a requisi√ß√£o**:
    - O servidor selecionado processa a requisi√ß√£o e retorna a resposta ao Load Balancer.
4. **Resposta ao cliente**:
    - O Load Balancer repassa a resposta ao cliente.

---

### **Algoritmos de Balanceamento Comuns**

- **Round Robin**:
    - Cada servidor recebe requisi√ß√µes de forma sequencial.
- **Least Connections**:
    - Redireciona para o servidor com menos conex√µes ativas.
- **IP Hash**:
    - Redireciona tr√°fego com base no hash do IP do cliente.
- **Weighted Round Robin**:
    - Cada servidor recebe tr√°fego proporcional a um peso configurado.

---

### **Vantagens do Load Balancer**

- **Maior Resili√™ncia**: Ele evita que falhas em um √∫nico servidor afetem o sistema.
- **Melhor Experi√™ncia do Usu√°rio**: Ao distribuir a carga, reduz o tempo de resposta.
- **Escalabilidade Horizontal**: Permite adicionar mais servidores de forma eficiente.

---

### **Cen√°rios de Uso**

1. **Aplica√ß√µes Web**:
    - Distribuir tr√°fego HTTP/HTTPS entre v√°rias inst√¢ncias de aplica√ß√£o.
2. **Servi√ßos de Banco de Dados**:
    - Balancear conex√µes para r√©plicas de leitura.
3. **Aplica√ß√µes Globais**:
    - Direcionar usu√°rios para servidores em regi√µes geograficamente mais pr√≥ximas.
4. **Sistemas em Kubernetes**:
    - Load Balancer atua como ponto de entrada para o cluster (Ingress ou Service tipo LoadBalancer).

---

O **Load Balancer Controller** √© uma ferramenta utilizada em clusters Kubernetes para gerenciar automaticamente **Load Balancers** nos provedores de nuvem, como AWS, GCP e Azure. Ele √© respons√°vel por criar, configurar e gerenciar Load Balancers externos, de forma que as aplica√ß√µes expostas no cluster possam ser acessadas a partir de fora.

### **O que √© um Load Balancer Controller no Kubernetes?**

- √â um **Controlador Kubernetes** que gerencia recursos do tipo `Ingress` ou `Service` com a configura√ß√£o `type: LoadBalancer`.
- Automatiza a integra√ß√£o entre o cluster Kubernetes e os servi√ßos de balanceamento de carga do provedor de nuvem.
- Cria e ajusta Load Balancers com base nas defini√ß√µes no cluster, otimizando o tr√°fego para pods.

---

### **Exemplo no AWS: AWS Load Balancer Controller**

O **AWS Load Balancer Controller** gerencia dois tipos principais de Load Balancers no Amazon EKS:

1. **Application Load Balancer (ALB)**:
    - Usado para tr√°fego HTTP/HTTPS.
    - Suporta balanceamento baseado em URL, cabe√ßalhos ou outras regras da camada 7.
2. **Network Load Balancer (NLB)**:
    - Usado para tr√°fego TCP/UDP de baixa lat√™ncia.
    - Funciona na camada 4 (rede) do modelo OSI.

### Funcionalidades:

- Configura√ß√£o autom√°tica de ALB/NLB com base nos recursos `Ingress` e `Service`.
- Suporte a regras avan√ßadas como roteamento baseado em caminhos (path-based routing).
- Integra√ß√£o com o **AWS Certificate Manager (ACM)** para configura√ß√£o de certificados SSL/TLS.
- Gerenciamento de seguran√ßa com grupos de seguran√ßa associados ao Load Balancer.

---

### **Como Funciona o Load Balancer Controller?**

1. **Deployment**:
    - O controlador √© implementado como um Deployment no cluster Kubernetes.
    - Ele monitora os recursos Kubernetes, como `Ingress` e `Service`, para identificar altera√ß√µes.
2. **Cria√ß√£o de Load Balancer**:
    - Quando um recurso `Ingress` ou `Service` √© criado ou atualizado, o controlador se comunica com o provedor de nuvem para provisionar um Load Balancer.
3. **Configura√ß√£o Autom√°tica**:
    - Configura os backends (grupos de inst√¢ncias ou pods), pol√≠ticas de roteamento e regras de seguran√ßa no Load Balancer.
4. **Manuten√ß√£o**:
    - Atualiza automaticamente o Load Balancer com base nas altera√ß√µes no cluster, como a adi√ß√£o ou remo√ß√£o de pods.

---

### **Componentes-Chave**

1. **Ingress Resource**:
    - Define regras de roteamento de tr√°fego HTTP/HTTPS para diferentes servi√ßos Kubernetes.
    - Exemplo:
        
        ```yaml
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: exemplo-ingress
          annotations:
            alb.ingress.kubernetes.io/scheme: internet-facing
        spec:
          rules:
          - host: exemplo.com
            http:
              paths:
              - path: /
                pathType: Prefix
                backend:
                  service:
                    name: meu-servico
                    port:
                      number: 80
        
        ```
        
2. **Service of Type LoadBalancer**:
    - Provisione um Load Balancer automaticamente ao expor um servi√ßo.
    - Exemplo:
        
        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: meu-servico
        spec:
          type: LoadBalancer
          ports:
          - port: 80
            targetPort: 8080
          selector:
            app: minha-aplicacao
        
        ```
        
3. **Annotations**:
    - Configuram comportamentos espec√≠ficos, como:
        - Tipo de Load Balancer (ALB ou NLB).
        - Certificados SSL.
        - Regras de roteamento.

---

### **Benef√≠cios do Load Balancer Controller**

1. **Automa√ß√£o**:
    - Simplifica a configura√ß√£o e o gerenciamento de Load Balancers no Kubernetes.
2. **Integra√ß√£o Nativa**:
    - Usa servi√ßos gerenciados pelo provedor de nuvem, otimizando desempenho e custo.
3. **Escalabilidade**:
    - Ajusta automaticamente o Load Balancer com base nas mudan√ßas do cluster, como aumento ou diminui√ß√£o de pods.
4. **Melhor Gest√£o de Recursos**:
    - Reduz a necessidade de interven√ß√£o manual para gerenciar Load Balancers externos.

---

### **Exemplo de Configura√ß√£o do AWS Load Balancer Controller**

1. **Instale o controlador** usando `kubectl` e `helm`:
    
    ```bash
    helm repo add eks https://aws.github.io/eks-charts
    helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
        --set clusterName=<nome-do-cluster> \
        --set serviceAccount.create=false \
        --set serviceAccount.name=aws-load-balancer-controller \
        --namespace kube-system
    
    ```
    
2. **Configure um Ingress Resource** com regras para o ALB:
    
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: example-alb
      namespace: default
      annotations:
        alb.ingress.kubernetes.io/scheme: internet-facing
        alb.ingress.kubernetes.io/target-type: ip
    spec:
      rules:
      - host: example.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: example-service
                port:
                  number: 80
    
    ```
    

---

### **Principais Load Balancer Controllers**

1. **AWS Load Balancer Controller**:
    - Gerencia ALB e NLB no Amazon EKS.
2. **GCP Ingress Controller**:
    - Cria Load Balancers globais gerenciados no Google Cloud.
3. **Azure Application Gateway Ingress Controller**:
    - Gerencia Application Gateways na Azure.
4. **MetalLB** (On-Premises):
    - Fornece suporte a Load Balancers em clusters Kubernetes fora da nuvem.

---

### Instalando o Load Balancer Controller

1. Download do IAM Policy

```bash
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
```

1. Vamos executar um comando para criar uma policy

```bash
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --policy-document file://iam_policy.json
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2080.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2081.png)

### Agora, nessa pr√≥xima etapa realizaremos a configur√ß√£o do Service Account e do IAM Role

1. Service Account

```bash
eksctl create iamserviceaccount \
  --cluster=comunidadedevops-eks \
  --namespace=kube-system \
  --name=aws-load-balancer-controller \
  --role-name AmazonEKSLoadBalancerControllerRole \
  --attach-policy-arn=arn:aws:iam::654654576685:policy/AWSLoadBalancerControllerIAMPolicy \
  --approve
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2082.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2083.png)

### Instalar AWS Load Balancer Controller via Helm

1. Adicione o reposit√≥rio de chart do Helm `eks-charts`. A AWS mant√©m [esse reposit√≥rio](https://github.com/aws/eks-charts) no GitHub.

```bash
helm repo add eks https://aws.github.io/eks-charts
```

1. Atualize o reposit√≥rio local para confirmar que voc√™ tem os gr√°ficos mais recentes.

```bash
helm repo update eks
```

1. Instale o AWS Load Balancer Controller.

```bash
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=comunidadedevops-eks \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller
```

Deu sucesso:

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2084.png)

```bash
kubectl get po -n kube-system
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2085.png)

E depois consulte os logs desse dois Pods.

## Limpando  tudo

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2086.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2087.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2088.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2089.png)

Depois de cluster e do Node Group deletado, vamos deletar os Nat Gateways

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2090.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2091.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2092.png)

Agora vamos deletar a VPC, ao deletar a mesma perceba que ela ir√° deletar tudo que est√° relacionada a ela

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2093.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2094.png)

Agora vamos deletar o elastic ip

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2095.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2096.png)

## Introdu√ß√£o ao terraform e in√≠cio do projeto

## Arquitetura do terraform

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2097.png)

O **Terraform** √© uma ferramenta de infraestrutura como c√≥digo (IaC) que permite criar, gerenciar e versionar recursos em ambientes de nuvem e locais. A arquitetura do Terraform √© baseada em um modelo declarativo e possui os seguintes componentes principais:

---

### **1. Configura√ß√µes do Terraform**

Os arquivos de configura√ß√£o (geralmente com extens√£o `.tf` ou `.tf.json`) cont√™m o c√≥digo declarativo que define a infraestrutura desejada. Voc√™ descreve os recursos e suas propriedades, e o Terraform √© respons√°vel por criar e manter esses recursos.

---

### **2. CLI do Terraform**

A Interface de Linha de Comando (CLI) √© usada para interagir com o Terraform. Os comandos mais comuns incluem:

- `terraform init`: inicializa o diret√≥rio do projeto, baixando provedores e m√≥dulos necess√°rios.
- `terraform plan`: gera um plano de execu√ß√£o para mostrar as altera√ß√µes que ser√£o feitas.
- `terraform apply`: aplica as mudan√ßas descritas no plano.
- `terraform destroy`: destr√≥i a infraestrutura gerenciada pelo Terraform.

---

### **3. Providers**

Os **providers** s√£o plug-ins que permitem ao Terraform interagir com APIs espec√≠ficas, como AWS, Azure, Google Cloud, Kubernetes, entre outros. Eles traduzem os recursos declarados no c√≥digo para chamadas reais √† API do provedor.

---

### **4. State (Estado do Terraform)**

O **estado** √© onde o Terraform armazena informa√ß√µes sobre os recursos criados. Ele √© usado para:

- Rastrear mudan√ßas na infraestrutura.
- Determinar o que precisa ser atualizado, criado ou destru√≠do.
- Sincronizar a configura√ß√£o do Terraform com os recursos reais.

O estado pode ser armazenado localmente (`terraform.tfstate`) ou remotamente (em buckets S3, GCS ou outros backends). O uso de estado remoto √© recomendado em equipes para evitar conflitos.

---

### **5. M√≥dulos**

Os **m√≥dulos** s√£o agrupamentos reutiliz√°veis de configura√ß√µes do Terraform. Eles ajudam a organizar e compartilhar c√≥digo, tornando a infraestrutura mais modular e escal√°vel. Um m√≥dulo pode ser usado para encapsular a configura√ß√£o de um recurso espec√≠fico ou uma parte da infraestrutura.

---

### **6. Workspace**

Os **workspaces** permitem que voc√™ mantenha m√∫ltiplos estados dentro do mesmo conjunto de configura√ß√µes. Isso √© √∫til para gerenciar diferentes ambientes (como `dev`, `staging` e `prod`) com o mesmo c√≥digo.

---

### **Fluxo de Trabalho do Terraform**

1. **Escrever Configura√ß√µes:** Crie o c√≥digo declarativo para descrever os recursos.
2. **Inicializar o Projeto:** Execute `terraform init` para configurar os provedores e preparar o ambiente.
3. **Planejar:** Use `terraform plan` para visualizar as mudan√ßas que ser√£o aplicadas.
4. **Aplicar:** Execute `terraform apply` para criar ou atualizar os recursos.
5. **Gerenciar Infraestrutura:** Continue ajustando as configura√ß√µes e aplicando mudan√ßas conforme necess√°rio.
6. **Destruir:** Quando n√£o precisar mais dos recursos, use `terraform destroy`.

---

### **Vantagens da Arquitetura do Terraform**

- **Declaratividade:** Voc√™ define o estado desejado da infraestrutura, e o Terraform cuida dos detalhes de implementa√ß√£o.
- **Reutiliza√ß√£o:** M√≥dulos e configura√ß√µes reutiliz√°veis facilitam a manuten√ß√£o e o compartilhamento.
- **Idempot√™ncia:** Aplicar a mesma configura√ß√£o v√°rias vezes resulta no mesmo estado final.
- **Portabilidade:** Suporte a m√∫ltiplos provedores de nuvem e infraestrutura local.
- **Automa√ß√£o:** Facilita a integra√ß√£o com pipelines de CI/CD para gerenciar infraestrutura de forma cont√≠nua.

Essa arquitetura torna o Terraform uma ferramenta poderosa e amplamente adotada no mundo DevOps.

## Instala√ß√£o do Terraform

```bash
brew tap hashicorp/tap
brew install hashicorp/tap/terraform

terraform -version
```

## Terraform Workflow

O **fluxo de trabalho do Terraform** √© um processo bem definido para criar, atualizar e gerenciar a infraestrutura como c√≥digo. Ele segue etapas claras para garantir consist√™ncia e rastreamento das mudan√ßas na infraestrutura. Abaixo est√£o as etapas principais do fluxo de trabalho:

---

### **1. Escrever Configura√ß√µes**

- O processo come√ßa com a defini√ß√£o dos recursos que voc√™ deseja criar, utilizando o c√≥digo declarativo do Terraform em arquivos `.tf`.
- Exemplo de c√≥digo para criar uma inst√¢ncia no AWS:
    
    ```hcl
    provider "aws" {
      region = "us-east-1"
    }
    
    resource "aws_instance" "example" {
      ami           = "ami-12345678"
      instance_type = "t2.micro"
    }
    
    ```
    
- Voc√™ pode usar m√≥dulos para organizar e reutilizar configura√ß√µes complexas.

---

### **2. Inicializar o Projeto**

- O comando `terraform init` √© usado para preparar o ambiente de trabalho:
**Exemplo:**
    - Baixa e instala os provedores necess√°rios.
    - Configura backends para o estado remoto (se configurado).
    - Verifica se o diret√≥rio do projeto est√° configurado corretamente.
    
    ```bash
    terraform init
    
    ```
    

---

### **3. Planejar as Mudan√ßas**

- Use o comando `terraform plan` para gerar um plano de execu√ß√£o que detalha as mudan√ßas que o Terraform far√° na infraestrutura.
- O plano compara o estado atual com o desejado (definido no c√≥digo) e mostra:
    - Recursos a serem **criados**.
    - Recursos a serem **atualizados**.
    - Recursos a serem **destru√≠dos**.
    
    **Exemplo:**
    
    ```bash
    terraform plan
    
    ```
    

---

### **4. Aplicar as Mudan√ßas**

- Execute o comando `terraform apply` para aplicar o plano gerado e provisionar os recursos.
- O Terraform pedir√° uma confirma√ß√£o antes de aplicar as mudan√ßas, a menos que voc√™ use a op√ß√£o `auto-approve`.
    
    **Exemplo:**
    
    ```bash
    terraform apply
    
    ```
    
- Ap√≥s a execu√ß√£o, o Terraform atualizar√° o arquivo de estado (`terraform.tfstate`) para refletir as mudan√ßas feitas.

---

### **5. Inspecionar e Validar**

- Ap√≥s aplicar, voc√™ pode inspecionar o estado atual da infraestrutura usando o comando `terraform show`.
- Tamb√©m √© poss√≠vel usar `terraform validate` para verificar se os arquivos de configura√ß√£o est√£o corretos e bem formatados.

---

### **6. Ajustar e Iterar**

- Quando surgirem novas demandas ou altera√ß√µes na infraestrutura:
    1. Atualize os arquivos de configura√ß√£o.
    2. Gere um novo plano com `terraform plan`.
    3. Aplique as mudan√ßas com `terraform apply`.

---

### **7. Destruir a Infraestrutura**

- Quando os recursos n√£o forem mais necess√°rios, use o comando `terraform destroy` para desprovision√°-los.
- Isso ajuda a evitar custos desnecess√°rios em provedores de nuvem.
    
    **Exemplo:**
    
    ```bash
    terraform destroy
    
    ```
    

---

### **Fluxo Resumido**

1. **Configura√ß√£o:** Crie ou edite os arquivos `.tf`.
2. **Inicializa√ß√£o:** `terraform init` para configurar o ambiente.
3. **Planejamento:** `terraform plan` para visualizar mudan√ßas.
4. **Aplica√ß√£o:** `terraform apply` para provisionar recursos.
5. **Gerenciamento cont√≠nuo:** Ajuste e re-aplique conforme necess√°rio.
6. **Destrui√ß√£o:** `terraform destroy` para remover recursos.

---

### **Boas Pr√°ticas no Workflow**

- **Versionamento de C√≥digo:** Use sistemas de controle de vers√£o, como Git, para rastrear mudan√ßas nos arquivos `.tf`.
- **Estado Remoto:** Configure um backend remoto para armazenar o estado compartilhado em equipes.
- **Automa√ß√£o:** Integre o Terraform em pipelines de CI/CD para automa√ß√£o do fluxo de trabalho.
- **Valida√ß√£o:** Execute `terraform validate` e `terraform fmt` antes de aplicar mudan√ßas.
- **M√≥dulos Reutiliz√°veis:** Estruture configura√ß√µes em m√≥dulos para aumentar a efici√™ncia.

---

Esse fluxo garante que as mudan√ßas sejam planejadas, documentadas e executadas de maneira previs√≠vel e segura.

Segue a documenta√ß√£o para mais informa√ß√µes:

https://developer.hashicorp.com/terraform/intro/core-workflow

## Setup do reposit√≥rio no GitHub

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2098.png)

## Providers

No **Terraform**, os **providers** s√£o componentes fundamentais que permitem a intera√ß√£o com APIs de plataformas e servi√ßos para provisionar e gerenciar recursos. Eles atuam como intermedi√°rios que traduzem a configura√ß√£o declarativa do Terraform em chamadas de API que criam, atualizam ou destroem os recursos em um provedor espec√≠fico.

---

### **Caracter√≠sticas Principais dos Providers**

1. **Interface com Provedores de Servi√ßo:**
    - Os providers conectam o Terraform a provedores de nuvem, servi√ßos SaaS, bancos de dados ou at√© sistemas locais.
    - Exemplos de provedores: AWS, Azure, Google Cloud, Kubernetes, GitHub, etc.
2. **Recursos Espec√≠ficos:**
    - Cada provider oferece um conjunto de recursos e funcionalidades que podem ser gerenciados.
    - Por exemplo:
        - **AWS:** EC2, S3, RDS, Lambda, etc.
        - **Azure:** VMs, Blob Storage, Azure Functions, etc.
        - **Kubernetes:** Deployments, Pods, Services, etc.
3. **Configura√ß√£o Personalizada:**
    - Os providers precisam ser configurados com informa√ß√µes como credenciais de acesso e regi√£o.
    - Exemplo de configura√ß√£o para o provider AWS:
        
        ```hcl
        provider "aws" {
          region = "us-east-1"
          access_key = "your-access-key"
          secret_key = "your-secret-key"
        }
        
        ```
        
4. **M√≥dulos Plug√°veis:**
    - Providers s√£o m√≥dulos plug√°veis, baixados durante a execu√ß√£o de `terraform init`.
    - Isso garante que o Terraform use a vers√£o correta do provider para o projeto.

---

### **Configura√ß√£o de Providers**

A configura√ß√£o dos providers √© declarada no arquivo `.tf` para informar ao Terraform qual plataforma ser√° gerenciada.

**Exemplo para m√∫ltiplos providers:**

```hcl
provider "aws" {
  region = "us-west-1"
}

provider "google" {
  credentials = file("path-to-credentials.json")
  project     = "my-gcp-project"
  region      = "us-central1"
}

```

---

### **Vers√£o dos Providers**

- √â recomend√°vel especificar a vers√£o do provider para evitar atualiza√ß√µes n√£o intencionais que possam introduzir altera√ß√µes incompat√≠veis.
- Isso √© feito com o bloco `required_providers`.

**Exemplo:**

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

```

---

### **Como os Providers Funcionam no Terraform Workflow**

1. **Download:** O comando `terraform init` baixa os providers necess√°rios para o projeto.
2. **Intera√ß√£o com APIs:** Durante o `terraform apply`, o Terraform usa o provider para interagir com a API do servi√ßo e gerenciar os recursos.
3. **Sincroniza√ß√£o:** O estado do Terraform √© atualizado conforme o feedback do provider sobre os recursos criados ou modificados.

---

### **Principais Providers Populares**

- **Nuvens P√∫blicas:**
    - AWS (`hashicorp/aws`)
    - Azure (`hashicorp/azurerm`)
    - Google Cloud (`hashicorp/google`)
- **Orquestradores:**
    - Kubernetes (`hashicorp/kubernetes`)
    - Docker (`kreuzwerker/docker`)
- **Sistemas SaaS:**
    - GitHub (`integrations/github`)
    - Datadog (`DataDog/datadog`)
- **Infraestrutura Local:**
    - VMware vSphere (`hashicorp/vsphere`)
    - OpenStack (`terraform-provider-openstack/openstack`)

---

### **Import√¢ncia dos Providers**

- Permitem que o Terraform seja uma ferramenta **multicloud** e **multiprop√≥sito**.
- Ampliam a flexibilidade para gerenciar desde infraestrutura na nuvem at√© servi√ßos SaaS e sistemas locais.
- Abstraem a complexidade das APIs, tornando o gerenciamento mais simples e declarativo.

---

Resumindo, os **providers** s√£o os pilares do Terraform que possibilitam sua ampla compatibilidade com diferentes plataformas, fazendo a ponte entre o c√≥digo declarativo e os recursos reais provisionados.

√â muito importante definir a vers√£o dos providers.

Segue a documenta√ß√£o dos providers:

https://registry.terraform.io/browse/providers

Nesse m√≥dulo vamos utilizar o provider do AWS.

Documentation do provider do AWS:

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%2099.png)

Implementando provider da AWS:

```hcl
terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.81.0"
    }
  }
}

provider "aws" {
  # Configuration options
  region
}
```

Execute o seguinte comando para iniciar e baixar os plugins do provider da AWS na vers√£o mais recente:

```hcl
terraform init
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20100.png)

üóÑ OBS! üóÑ O seu dia-a-dia para montar configura√ß√µes do terraform √© de suma import√¢ncia que leia a documenta√ß√£o, e que ela esteja sempre aberta.

## Primeiro Recurso VPC

Criando TF do VPC

```hcl
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
}
```

Nesse `main`, eu posso colocar um apelido

Execute o seguinte comando para ler os c√≥digos e montar plano e mostrar na tela o que foi executado.

```hcl
terraform plan
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20101.png)

Agora vamos aplicar essas configura√ß√µes com o comando:

```hcl
terraform apply
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20102.png)

Agora v√° no sua conta da AWS e veja se a VPC foi criada.

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20103.png)

Agora v√° em Arguments references e veja qual referencia sr√° o idela para configura√ß√£o no TF.

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20104.png)

```hcl
resource "aws_vpc" "eks_vpc" {
  cidr_block = "10.0.0.0/16"
  # Habilitando DNS Support e DNS Hostnames
  enable_dns_support = true
  enable_dns_hostnames = true

  # Implementando tags
  tags = {
    Name = "comunidadedevops-vpc"
  }

}
```

```hcl
terraform plan

terraform apply -auto-approve
# Esse -auto-approve indica que voc√™ n√£o precisa colocar 'yes' 
# ap√≥s executar esse comando
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20105.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20106.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20107.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20108.png)

## Terraform state

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20109.png)

Esse arquivo que foi gerado o terraform.tfstate √© um arquivo muito cr√≠tico, ou seja, tome muito cuidado com ele.

Ele deve ser aramazenado em  repositorio externo.

## Migrando o Backend do tfstate

O backend mais utilizado para aramazenar o tfstate √© o S3  do AWS

Vamos criar um bucket S3 no AWS

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20110.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20111.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20112.png)

```hcl
aws s3 ls

aws s3 ls s3://comunidadedevops-eks-s3
```

```hcl
terraform {
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.81.0"
    }
  }
  backend "s3" {
    bucket = "comunidadedevops-eks-s3"
    key    = "dev/terraform.tfstate"
    region = "us-east-1"
  }

}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}
```

```hcl
terraform init -migrate-state
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20113.png)

Ou seja, toda modifica√ß√£o que eu for fazer as altera√ß√µes do tfstate ser√£o armazenadas no bucket criado na AWS

## terraform fmt

√â sempre importante formatar os arquivos do terraform

```bash
terraform fmt -recursive -check

terraform fmt -recursive
```

As linhas de comando mencionadas est√£o relacionadas ao comando `terraform fmt`, que √© usado para formatar arquivos de configura√ß√£o do Terraform (`.tf` e `.tf.json`). Ele garante que o c√≥digo segue os padr√µes de estilo recomendados pelo Terraform, tornando-o mais leg√≠vel e consistente.

---

### **1. `terraform fmt -recursive -check`**

- **Descri√ß√£o:**
    - Esse comando verifica todos os arquivos de configura√ß√£o dentro do diret√≥rio atual e seus subdiret√≥rios, sem modificar nada.
    - O argumento `check` √© usado para realizar uma verifica√ß√£o e informar se algum arquivo n√£o est√° formatado corretamente.
- **Uso pr√°tico:**
    - √â √∫til em pipelines de CI/CD para validar que o c√≥digo do Terraform segue o formato padr√£o antes de permitir o merge ou a execu√ß√£o.
- **Sa√≠da esperada:**
    - Caso os arquivos estejam corretamente formatados, o comando n√£o exibe nenhuma mensagem.
    - Caso haja arquivos fora do padr√£o, o comando lista os arquivos que precisam ser formatados.
- **Exemplo de sa√≠da:**
    
    ```
    main.tf
    variables.tf
    
    ```
    
    Isso indica que os arquivos `main.tf` e `variables.tf` precisam ser formatados.
    

---

### **2. `terraform fmt -recursive`**

- **Descri√ß√£o:**
    - Esse comando formata todos os arquivos de configura√ß√£o no diret√≥rio atual e em seus subdiret√≥rios, aplicando as corre√ß√µes necess√°rias.
    - Ele modifica diretamente os arquivos para ajust√°-los ao estilo padr√£o do Terraform.
- **Uso pr√°tico:**
    - Ideal para garantir a consist√™ncia de formata√ß√£o antes de commits ou durante o desenvolvimento.
- **Comportamento:**
    - Apenas os arquivos que precisam de ajustes ser√£o alterados.
    - Se todos os arquivos j√° estiverem formatados corretamente, o comando n√£o faz nenhuma mudan√ßa nem exibe mensagens.
- **Exemplo:**
Se voc√™ tem um arquivo com espa√ßamento ou indenta√ß√£o fora do padr√£o, como:
    
    ```hcl
    resource "aws_instance" "example" {
        ami = "ami-12345678"
      instance_type = "t2.micro"
    }
    
    ```
    
    Ap√≥s executar o comando, ele ser√° corrigido para:
    
    ```hcl
    resource "aws_instance" "example" {
      ami           = "ami-12345678"
      instance_type = "t2.micro"
    }
    
    ```
    

---

### **Diferen√ßa entre os dois comandos**

| Comando | A√ß√£o |
| --- | --- |
| `terraform fmt -recursive` | Formata automaticamente os arquivos de configura√ß√£o, aplicando corre√ß√µes diretamente. |
| `terraform fmt -recursive -check` | Apenas verifica os arquivos sem modificar nada, √∫til para valida√ß√£o em pipelines ou revis√µes. |

---

### **Cen√°rio Pr√°tico: CI/CD**

- **`terraform fmt -recursive -check`:** Adicione este comando em um pipeline de integra√ß√£o cont√≠nua para garantir que todos os arquivos estejam formatados corretamente.
- **`terraform fmt -recursive`:** Use localmente para ajustar a formata√ß√£o antes de enviar o c√≥digo para o reposit√≥rio.

Esses comandos ajudam a manter um padr√£o consistente de formata√ß√£o no c√≥digo do Terraform, essencial para colabora√ß√£o em equipe e manuten√ß√£o de longo prazo.

## Pre-commit hooks para terraform

Os **pre-commit hooks** para Terraform s√£o scripts automatizados que s√£o executados antes de um commit ser feito no seu reposit√≥rio Git. Eles ajudam a garantir a qualidade e a consist√™ncia do c√≥digo, al√©m de prevenir que c√≥digo com problemas seja enviado ao reposit√≥rio. No contexto do Terraform, esses hooks geralmente verificam a formata√ß√£o, a validade, e outros padr√µes relacionados aos arquivos de configura√ß√£o (`.tf`).

---

### **Como Funcionam os Pre-commit Hooks?**

1. **Defini√ß√£o de Regras:** Voc√™ configura os hooks no arquivo `.pre-commit-config.yaml`, onde define as ferramentas e as verifica√ß√µes a serem realizadas.
2. **Execu√ß√£o Autom√°tica:** Sempre que voc√™ tenta fazer um commit, o `pre-commit` executa essas verifica√ß√µes antes que o commit seja conclu√≠do.
3. **Corre√ß√£o ou Falha:**
    - Se o c√≥digo passar em todas as verifica√ß√µes, o commit √© feito.
    - Se houver problemas, os hooks falham e fornecem mensagens de erro, permitindo que voc√™ corrija antes de tentar novamente.

---

### **Por que usar Pre-commit Hooks para Terraform?**

- **Consist√™ncia:** Garante que o c√≥digo est√° sempre formatado (`terraform fmt`) e segue os padr√µes de estilo.
- **Valida√ß√£o:** Verifica a validade das configura√ß√µes do Terraform (`terraform validate`).
- **Qualidade:** Previne erros comuns ao aplicar pr√°ticas recomendadas automaticamente.
- **Automa√ß√£o:** Automatiza tarefas repetitivas, economizando tempo e esfor√ßo.

---

### **Exemplo de Configura√ß√£o do Pre-commit para Terraform**

### **Passo 1: Instalar o Pre-commit**

Instale o pre-commit usando `pip`:

```bash
pip install pre-commit

```

### **Passo 2: Criar o Arquivo `.pre-commit-config.yaml`**

Exemplo de configura√ß√£o b√°sica para Terraform:

```yaml
repos:
  - repo: https://github.com/antonbabenko/pre-commit-terraform
    rev: v1.77.0  # Use a vers√£o mais recente
    hooks:
      - id: terraform_fmt
      - id: terraform_validate
      - id: terraform_docs

```

### **Passo 3: Instalar os Hooks**

Depois de criar o arquivo de configura√ß√£o, instale os hooks no reposit√≥rio:

```bash
pre-commit install

```

### **Passo 4: Testar os Hooks**

Teste os hooks executando o pre-commit manualmente:

```bash
pre-commit run --all-files

```

---

### **Hooks Comuns para Terraform**

Os hooks mais populares para Terraform incluem:

| **Hook** | **Descri√ß√£o** |
| --- | --- |
| `terraform_fmt` | Garante que os arquivos `.tf` est√£o formatados corretamente. |
| `terraform_validate` | Valida os arquivos do Terraform para garantir que a sintaxe e as depend√™ncias est√£o corretas. |
| `terraform_docs` | Atualiza automaticamente a documenta√ß√£o gerada nos blocos de vari√°veis e outputs. |
| `checkov` | Faz uma an√°lise de seguran√ßa nas configura√ß√µes do Terraform. |
| `tflint` | Verifica e aplica linting aos arquivos do Terraform para encontrar problemas ou inconsist√™ncias. |

---

### **Fluxo Pr√°tico de Uso**

1. Voc√™ edita os arquivos `.tf`.
2. Ao tentar fazer o commit (`git commit`), o pre-commit hooks:
    - Formata o c√≥digo.
    - Valida os arquivos.
    - Atualiza documenta√ß√£o ou realiza outras verifica√ß√µes configuradas.
3. Se tudo estiver correto, o commit √© conclu√≠do. Caso contr√°rio, ele falha com mensagens explicativas.

---

### **Benef√≠cios de Usar Pre-commit Hooks com Terraform**

- **Preven√ß√£o de Erros:** Evita que erros b√°sicos cheguem ao reposit√≥rio.
- **Automa√ß√£o de Qualidade:** Reduz revis√µes manuais relacionadas √† formata√ß√£o e padr√µes.
- **Colabora√ß√£o:** Garante que todos os membros da equipe seguem os mesmos padr√µes.
- **Melhoria Cont√≠nua:** Facilita a ado√ß√£o de boas pr√°ticas e ferramentas ao longo do tempo.

---

### **Integra√ß√£o com CI/CD**

Os pre-commit hooks podem ser integrados a pipelines de CI/CD para garantir que o c√≥digo no reposit√≥rio esteja sempre em conformidade, mesmo que algum membro da equipe n√£o tenha os hooks configurados localmente.

Essa pr√°tica, combinada com o Terraform, melhora significativamente a efici√™ncia, a seguran√ßa e a qualidade do c√≥digo.

Segue a documenta√ß√£o do pr√©-commit:

https://github.com/antonbabenko/pre-commit-terraform?tab=readme-ov-file#how-to-install

**pre-commit para o comando terraform fmt:**

```yaml
repos:
- repo: https://github.com/antonbabenko/pre-commit-terraform
  rev: v1.96.3 # Get the latest from: https://github.com/antonbabenko/pre-commit-terraform/releases
  hooks:
    - id: terraform_fmt
      args:
      - --args=-recursive
```

```yaml
pre-commit intall

# output
pre-commit installed at .git/hooks/pre-commit
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20114.png)

## Terraform validate

O comando `terraform validate` √© usado para validar a sintaxe e a configura√ß√£o dos arquivos de configura√ß√£o do Terraform. Ele verifica se os arquivos de configura√ß√£o est√£o corretos e se todas as vari√°veis e recursos est√£o definidos corretamente. Este comando n√£o faz nenhuma altera√ß√£o na infraestrutura, apenas verifica a validade da configura√ß√£o.

### Exemplo de uso:

```
terraform validate

```

### Passos:

1. Navegue at√© o diret√≥rio onde est√£o os arquivos de configura√ß√£o do Terraform.
2. Execute o comando `terraform validate`.
3. O Terraform ir√° analisar os arquivos e retornar mensagens de erro ou sucesso.

### Benef√≠cios:

- Ajuda a identificar erros de sintaxe antes de aplicar as mudan√ßas.
- Garante que a configura√ß√£o est√° correta e completa.

### Exemplo de sa√≠da:

```
Success! The configuration is valid.

```

Se houver erros, o Terraform fornecer√° detalhes sobre o que est√° errado para que voc√™ possa corrigir.

### Implementando o comando terraform validate no pre commit

```yaml
repos:
- repo: https://github.com/antonbabenko/pre-commit-terraform
  rev: v1.96.3 # Get the latest from: https://github.com/antonbabenko/pre-commit-terraform/releases
  hooks:
    - id: terraform_fmt
      args:
      - --args=-recursive
    - id: terraform_validate
```

## Terraform console

O comando `terraform console` abre um console interativo que permite avaliar express√µes e testar consultas no contexto da configura√ß√£o do Terraform. √â √∫til para depurar e explorar a configura√ß√£o do Terraform.

### Exemplo de uso:

```
terraform console

```

### Passos:

1. Navegue at√© o diret√≥rio onde est√£o os arquivos de configura√ß√£o do Terraform.
2. Execute o comando `terraform console`.
3. O console interativo ser√° aberto, permitindo que voc√™ insira express√µes Terraform.

### Exemplos de comandos no console:

- Avaliar uma vari√°vel:
    
    ```hcl
    var.my_variable
    
    ```
    
- Consultar um recurso:
    
    ```hcl
    aws_instance.my_instance.id
    
    ```
    
- Usar fun√ß√µes do Terraform:
    
    ```hcl
    length(var.my_list)
    
    ```
    

### Benef√≠cios:

- Permite testar e validar express√µes sem aplicar mudan√ßas.
- Facilita a depura√ß√£o de configura√ß√µes complexas.
- Ajuda a entender como as vari√°veis e recursos s√£o avaliados.

Para sair do console, voc√™ pode usar o comando `exit` ou pressionar `Ctrl+D`.

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20115.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20116.png)

## Terraform destroy

O comando `terraform destroy` √© usado para destruir a infraestrutura gerenciada pelo Terraform. Ele remove todos os recursos definidos na configura√ß√£o do Terraform.

### Exemplo de uso:

```
terraform destroy

```

### Passos:

1. Navegue at√© o diret√≥rio onde est√£o os arquivos de configura√ß√£o do Terraform.
2. Execute o comando `terraform destroy`.
3. O Terraform ir√° mostrar um plano de destrui√ß√£o e solicitar confirma√ß√£o.
4. Digite `yes` para confirmar e iniciar a destrui√ß√£o dos recursos.

### Benef√≠cios:

- √ötil para limpar recursos de um ambiente de teste ou desenvolvimento.
- Ajuda a evitar custos desnecess√°rios ao remover recursos n√£o utilizados.

### Exemplo de sa√≠da:

```
Terraform will perform the following actions:

  # aws_instance.example will be destroyed
  - resource "aws_instance" "example" {
      ...
    }

Plan: 0 to add, 0 to change, 1 to destroy.

Do you really want to destroy all resources?
  Terraform will destroy all your managed infrastructure, as shown above.
  There is no undo. Only 'yes' will be accepted to confirm.

  Enter a value: yes

aws_instance.example: Destroying... [id=i-1234567890abcdef0]
aws_instance.example: Destruction complete after 1m2s

Destroy complete! Resources: 1 destroyed.

```

### Avisos:

- Use com cuidado, pois este comando remove todos os recursos gerenciados pelo Terraform.
- Certifique-se de que voc√™ realmente deseja destruir os recursos antes de confirmar.

Similar code found with 2 license types

```yaml
terraform apply -destroy
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20117.png)

## Subnets

**Configurando o terraforn para implementar na minha cloud AWS**

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = "10.0.1.0/24"
  availability_zone       = "us-east-1a"
  map_public_ip_on_launch = true

  tags = {
    Name = "comunidadedevops-subnet-public-1a"
  }
}

resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = "10.0.2.0/24"
  availability_zone       = "us-east-1b"
  map_public_ip_on_launch = true

  tags = {
    Name = "comunidadedevops-subnet-public-1b"
  }
}
```

```hcl
terraform fmt
terraform plan
terraform apply -auto-approve
```

## Variables

```hcl
variable "cidr_block" {
    type        = string
    description = "Networking CIDR block to be used for the VPC"
}
```

```hcl
resource "aws_vpc" "eks_vpc" {
  # Habilitando DNS Support e DNS Hostnames
  enable_dns_support   = true
  enable_dns_hostnames = true
  cidr_block           = var.cidr_block

  # Implementando tags
  tags = {
    Name = "comunidadedevops-vpc"
  }

}
```

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 1)
  availability_zone       = "us-east-1a"
  map_public_ip_on_launch = true

  tags = {
    Name = "comunidadedevops-subnet-public-1a"
  }
}

resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 2)
  availability_zone       = "us-east-1b"
  map_public_ip_on_launch = true

  tags = {
    Name = "comunidadedevops-subnet-public-1b"
  }
}
```

```hcl
terraform plan -var 'cidr_block=10.0.0.0/16'
```

## terraform.tfvars (Valores para vari√°veis)

Crie um arquivo terraform.tfvars e implemente todos os valores das vari√°veis dentro desse arquivo

```hcl
# Aqui ir√° conter os valores das vari√°veis que ser√£o utilizadas no arquivo main.tf

cidr_block = "10.0.0.0/16"
```

## Region din√¢mica e tags para ALB

- Crie um arquivo chamado `region.tf`

```hcl
data "aws_region" "current" {}
```

### Explica√ß√£o do C√≥digo

Este c√≥digo define duas subnets p√∫blicas em uma VPC existente no AWS usando Terraform. Ambas as subnets s√£o configuradas para serem usadas com o Amazon EKS (Elastic Kubernetes Service).

### Detalhes do C√≥digo

1. **aws_subnet.eks_subnet_public_1a**:
    - **vpc_id**: ID da VPC onde a subnet ser√° criada.
    - **cidr_block**: Bloco CIDR da subnet, calculado dinamicamente usando a fun√ß√£o `cidrsubnet`.
    - **availability_zone**: Zona de disponibilidade onde a subnet ser√° criada, obtida dinamicamente.
    - **map_public_ip_on_launch**: Define se as inst√¢ncias lan√ßadas na subnet receber√£o IPs p√∫blicos automaticamente.
    - **tags**: Tags associadas √† subnet, incluindo um nome e uma tag espec√≠fica para o Kubernetes.
2. **aws_subnet.eks_subnet_public_1b**:
    - Configura√ß√£o similar √† subnet `eks_subnet_public_1a`, mas em uma zona de disponibilidade diferente e com um bloco CIDR diferente.

### C√≥digo Completo

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 1)
  availability_zone       = "${data.aws_region.current.name}a"
  map_public_ip_on_launch = true

  tags = {
    Name                     = "comunidadedevops-subnet-public-1a",
    "kubernetes.io/role/elb" = 1
  }
}

resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 2)
  availability_zone       = "${data.aws_region.current.name}b"
  map_public_ip_on_launch = true

  tags = {
    Name                     = "comunidadedevops-subnet-public-1b",
    "kubernetes.io/role/elb" = 1
  }
}

```

### Pontos Importantes

- **Regi√£o Din√¢mica**: A regi√£o √© obtida dinamicamente usando `data.aws_region.current.name`.
- **CIDR Din√¢mico**: O bloco CIDR √© calculado dinamicamente usando a fun√ß√£o `cidrsubnet`.
- **Tags**: As tags s√£o usadas para identificar as subnets e para integra√ß√£o com o Kubernetes.

Este c√≥digo cria duas subnets p√∫blicas em diferentes zonas de disponibilidade, configuradas para uso com o Amazon EKS e com IPs p√∫blicos atribu√≠dos automaticamente √†s inst√¢ncias lan√ßadas.

```hcl
terraform fmt
terraform plan
terraform apply -auto-aprove
```

## Desacoplando tags usando locals

- Crie um arquivo chama local.tf, √© onde ser√° colocada todas as tegas que ser√£o utilizadas.

```hcl
locals {
  tags = {
    Department = "DevOps"
    Organization = "Infrastructure and Operations"
    Project = "EKS"
    Environment = "Development"
  }
}
```

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 1)
  availability_zone       = "${data.aws_region.current.name}a"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "comunidadedevops-subnet-public-1a",
    "kubernetes.io/role/elb" = 1
    }
  )
}

resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 2)
  availability_zone       = "${data.aws_region.current.name}b"
  map_public_ip_on_launch = true

   tags = merge(
    local.tags,
    {
      Name                     = "comunidadedevops-subnet-public-1b",
    "kubernetes.io/role/elb" = 1
    }
  )
}
```

### Explica√ß√£o do C√≥digo

Este c√≥digo define duas subnets p√∫blicas em uma VPC existente no AWS usando Terraform. Ambas as subnets s√£o configuradas para serem usadas com o Amazon EKS (Elastic Kubernetes Service).

### Detalhes do C√≥digo

### Subnet 1: `eks_subnet_public_1a`

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 1)
  availability_zone       = "${data.aws_region.current.name}a"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "comunidadedevops-subnet-public-1a",
      "kubernetes.io/role/elb" = 1
    }
  )
}

```

- **vpc_id**: ID da VPC onde a subnet ser√° criada.
- **cidr_block**: Bloco CIDR da subnet, calculado dinamicamente usando a fun√ß√£o `cidrsubnet`.
- **availability_zone**: Zona de disponibilidade onde a subnet ser√° criada, obtida dinamicamente.
- **map_public_ip_on_launch**: Define se as inst√¢ncias lan√ßadas na subnet receber√£o IPs p√∫blicos automaticamente.
- **tags**: Tags associadas √† subnet, incluindo um nome e uma tag espec√≠fica para o Kubernetes.

### Subnet 2: `eks_subnet_public_1b`

```hcl
resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 2)
  availability_zone       = "${data.aws_region.current.name}b"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "comunidadedevops-subnet-public-1b",
      "kubernetes.io/role/elb" = 1
    }
  )
}

```

- Configura√ß√£o similar √† subnet `eks_subnet_public_1a`, mas em uma zona de disponibilidade diferente e com um bloco CIDR diferente.

### Pontos Importantes

- **Regi√£o Din√¢mica**: A regi√£o √© obtida dinamicamente usando `data.aws_region.current.name`.
- **CIDR Din√¢mico**: O bloco CIDR √© calculado dinamicamente usando a fun√ß√£o `cidrsubnet`.
- **Tags**: As tags s√£o usadas para identificar as subnets e para integra√ß√£o com o Kubernetes.

### Benef√≠cios

- **Flexibilidade**: A configura√ß√£o √© adapt√°vel a diferentes regi√µes e zonas de disponibilidade.
- **Organiza√ß√£o**: As tags ajudam a identificar e gerenciar os recursos de forma eficiente.
- **Integra√ß√£o com Kubernetes**: As tags espec√≠ficas para Kubernetes (`kubernetes.io/role/elb`) facilitam a integra√ß√£o com o Amazon EKS.

Este c√≥digo cria duas subnets p√∫blicas em diferentes zonas de disponibilidade, configuradas para uso com o Amazon EKS e com IPs p√∫blicos atribu√≠dos automaticamente √†s inst√¢ncias lan√ßadas.

## Private subnets

```hcl
resource "aws_subnet" "eks_subnet_private_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 3)
  availability_zone       = "${data.aws_region.current.name}a"

  tags = merge(
    local.tags,
    {
      Name                     = "comunidadedevops-priv-subnet-1a",
      "kubernetes.io/role/internal-elb" = 1
    }
  )
}

resource "aws_subnet" "eks_subnet_private_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 4)
  availability_zone       = "${data.aws_region.current.name}b"

  tags = merge(
    local.tags,
    {
      Name                     = "comunidadedevops-priv-subnet-1b",
      "kubernetes.io/role/internal-elb" = 1
    }
  )
}
```

## Variable ‚Äúproject_name‚Äù

```hcl
variable "cidr_block" {
  type        = string
  description = "Networking CIDR block to be used for the VPC"
}

variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}
```

```hcl
# Aqui ir√° conter os valores das vari√°veis que ser√£o utilizadas no arquivo main.tf

# Bloco CIDR
cidr_block = "10.0.0.0/16"

# Nome do projeto
project_name = "eksdevelopment"
```

```hcl
resource "aws_subnet" "eks_subnet_private_1a" {
  vpc_id            = aws_vpc.eks_vpc.id
  cidr_block        = cidrsubnet(var.cidr_block, 8, 3)
  availability_zone = "${data.aws_region.current.name}a"

  tags = merge(
    local.tags,
    {
      Name                              = "${var.project_name}-priv-subnet-1a",
      "kubernetes.io/role/internal-elb" = 1
    }
  )
}

resource "aws_subnet" "eks_subnet_private_1b" {
  vpc_id            = aws_vpc.eks_vpc.id
  cidr_block        = cidrsubnet(var.cidr_block, 8, 4)
  availability_zone = "${data.aws_region.current.name}b"

  tags = merge(
    local.tags,
    {
      Name                              = "${var.project_name}-priv-subnet-1b",
      "kubernetes.io/role/internal-elb" = 1
    }
  )
}
```

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 1)
  availability_zone       = "${data.aws_region.current.name}a"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "${var.project_name}-pub-subnet-1a",
      "kubernetes.io/role/elb" = 1
    }
  )
}

resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 2)
  availability_zone       = "${data.aws_region.current.name}b"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "${var.project_name}-pub-subnet-1b",
      "kubernetes.io/role/elb" = 1
    }
  )
}
```

## Internet Gateway e Route Table

```hcl
resource "aws_internet_gateway" "eks_igw" {
  vpc_id = aws_vpc.eks_vpc.id

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-igw"
    }
  )

}
```

```hcl
resource "aws_internet_gateway" "eks_igw" {
  vpc_id = aws_vpc.eks_vpc.id

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-igw"
    }
  )

}

# Configurando a rota padr√£o para a internet gateway
resource "aws_route_table" "eks_public_route_table" {
  vpc_id = aws_vpc.eks_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.eks_igw.id
  }

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-pub-route-table"
    }
  )
}
```

```hcl
resource "aws_subnet" "eks_subnet_public_1a" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 1)
  availability_zone       = "${data.aws_region.current.name}a"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "${var.project_name}-pub-subnet-1a",
      "kubernetes.io/role/elb" = 1
    }
  )
}

resource "aws_subnet" "eks_subnet_public_1b" {
  vpc_id                  = aws_vpc.eks_vpc.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, 2)
  availability_zone       = "${data.aws_region.current.name}b"
  map_public_ip_on_launch = true

  tags = merge(
    local.tags,
    {
      Name                     = "${var.project_name}-pub-subnet-1b",
      "kubernetes.io/role/elb" = 1
    }
  )
}

# Associando as subnets p√∫blicas com a route table p√∫blica
# ADD na subnet A
resource "aws_route_table_association" "eks_rtb_assoc_1a" {
  subnet_id      = aws_subnet.eks_subnet_public_1a.id
  route_table_id = aws_route_table.eks_public_route_table.id
}

# ADD na subnet B
resource "aws_route_table_association" "eks_rtb_assoc_1b" {
  subnet_id      = aws_subnet.eks_subnet_public_1b.id
  route_table_id = aws_route_table.eks_public_route_table.id
}
```

- Depois de aplicar, v√° at√© o console da AWS e consulte se foi criado corretamente.

## NAT gateway e EIP

```hcl
resource "aws_eip" "eks_ngw_eip_1a" {

  # No curso foi usado o 'vpc = true' para criar um Elastic IP na VPC
  # Por√©m, ele est√° depreciado e n√£o e est√° sendo mais usado

  # O correto √© usar o 'domain = "vpc"' para criar um Elastic IP na VPC
  domain = "vpc"
  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-eip-1a"
    }
  )
}

resource "aws_eip" "eks_ngw_eip_1b" {

  # No curso foi usado o 'vpc = true' para criar um Elastic IP na VPC
  # Por√©m, ele est√° depreciado e n√£o e est√° sendo mais usado

  # O correto √© usar o 'domain = "vpc"' para criar um Elastic IP na VPC
  domain = "vpc"
  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-eip-1b"
    }
  )
}

resource "aws_nat_gateway" "eks_ngw_1a" {
  allocation_id = aws_eip.eks_ngw_eip_1a.id
  subnet_id     = aws_subnet.eks_subnet_public_1a.id

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-ngw-1a"
    }
  )

}

resource "aws_nat_gateway" "eks_ngw_1b" {
  allocation_id = aws_eip.eks_ngw_eip_1b.id
  subnet_id     = aws_subnet.eks_subnet_public_1b.id

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-ngw-1b"
    }
  )

}
```

## Private route tables

```hcl
resource "aws_eip" "eks_ngw_eip_1a" {

  # No curso foi usado o 'vpc = true' para criar um Elastic IP na VPC
  # Por√©m, ele est√° depreciado e n√£o e est√° sendo mais usado

  # O correto √© usar o 'domain = "vpc"' para criar um Elastic IP na VPC
  domain = "vpc"
  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-eip-1a"
    }
  )
}

resource "aws_eip" "eks_ngw_eip_1b" {

  # No curso foi usado o 'vpc = true' para criar um Elastic IP na VPC
  # Por√©m, ele est√° depreciado e n√£o e est√° sendo mais usado

  # O correto √© usar o 'domain = "vpc"' para criar um Elastic IP na VPC
  domain = "vpc"
  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-eip-1b"
    }
  )
}

resource "aws_nat_gateway" "eks_ngw_1a" {
  allocation_id = aws_eip.eks_ngw_eip_1a.id
  subnet_id     = aws_subnet.eks_subnet_public_1a.id

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-ngw-1a"
    }
  )

}

resource "aws_nat_gateway" "eks_ngw_1b" {
  allocation_id = aws_eip.eks_ngw_eip_1b.id
  subnet_id     = aws_subnet.eks_subnet_public_1b.id

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-ngw-1b"
    }
  )

}

resource "aws_route_table" "eks_private_route_table_1a" {
  vpc_id = aws_vpc.eks_vpc.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.eks_ngw_1a.id
  }

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-priv-route-table-1a"
    }
  )
}

resource "aws_route_table" "eks_private_route_table_1b" {
  vpc_id = aws_vpc.eks_vpc.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.eks_ngw_1b.id
  }

  tags = merge(
    local.tags,
    {
      Name = "${var.project_name}-priv-route-table-1b"
    }
  )
}
```

```hcl
resource "aws_subnet" "eks_subnet_private_1a" {
  vpc_id            = aws_vpc.eks_vpc.id
  cidr_block        = cidrsubnet(var.cidr_block, 8, 3)
  availability_zone = "${data.aws_region.current.name}a"

  tags = merge(
    local.tags,
    {
      Name                              = "${var.project_name}-priv-subnet-1a",
      "kubernetes.io/role/internal-elb" = 1
    }
  )
}

resource "aws_subnet" "eks_subnet_private_1b" {
  vpc_id            = aws_vpc.eks_vpc.id
  cidr_block        = cidrsubnet(var.cidr_block, 8, 4)
  availability_zone = "${data.aws_region.current.name}b"

  tags = merge(
    local.tags,
    {
      Name                              = "${var.project_name}-priv-subnet-1b",
      "kubernetes.io/role/internal-elb" = 1
    }
  )
}

resource "aws_route_table_association" "eks_rtb_assoc_priv_1a" {
  subnet_id      = aws_subnet.eks_subnet_private_1a.id
  route_table_id = aws_route_table.eks_private_route_table_1a.id
}

resource "aws_route_table_association" "eks_rtb_assoc_priv_1b" {
  subnet_id      = aws_subnet.eks_subnet_private_1b.id
  route_table_id = aws_route_table.eks_private_route_table_1b.id
}
```

## Introdu√ß√£o a terraform modules

## Introdu√ß√£o

### M√≥dulos do Terraform

Os m√≥dulos do Terraform s√£o blocos reutiliz√°veis de configura√ß√£o que agrupam v√°rios recursos relacionados. Eles ajudam a organizar e reutilizar c√≥digo, facilitando a manuten√ß√£o e a padroniza√ß√£o das infraestruturas.

### Benef√≠cios dos M√≥dulos

- **Reutiliza√ß√£o**: Permite reutilizar a mesma configura√ß√£o em diferentes projetos.
- **Organiza√ß√£o**: Ajuda a estruturar o c√≥digo de forma mais organizada e modular.
- **Manuten√ß√£o**: Facilita a manuten√ß√£o e atualiza√ß√£o da infraestrutura.
- **Padroniza√ß√£o**: Garante que as melhores pr√°ticas sejam seguidas de forma consistente.

### Estrutura de um M√≥dulo

Um m√≥dulo geralmente consiste em tr√™s arquivos principais:

- [**main.tf**](http://main.tf/): Define os recursos principais.
- [**variables.tf**](http://variables.tf/): Define as vari√°veis de entrada.
- [**outputs.tf**](http://outputs.tf/): Define as sa√≠das do m√≥dulo.

### Exemplo de M√≥dulo

### Estrutura de Diret√≥rios

```
modules/
  ‚îî‚îÄ‚îÄ vpc/
      ‚îú‚îÄ‚îÄ main.tf
      ‚îú‚îÄ‚îÄ variables.tf
      ‚îî‚îÄ‚îÄ outputs.tf

```

### [main.tf](http://main.tf/)

```hcl
resource "aws_vpc" "main" {
  cidr_block = var.cidr_block

  tags = {
    Name = var.vpc_name
  }
}

resource "aws_subnet" "public" {
  count                   = var.public_subnet_count
  vpc_id                  = aws_vpc.main.id
  cidr_block              = cidrsubnet(var.cidr_block, 8, count.index)
  map_public_ip_on_launch = true

  tags = {
    Name = "${var.vpc_name}-public-${count.index}"
  }
}

```

### 

[variables.tf](http://variables.tf/)

```hcl
variable "cidr_block" {
  description = "The CIDR block for the VPC"
  type        = string
}

variable "vpc_name" {
  description = "The name of the VPC"
  type        = string
}

variable "public_subnet_count" {
  description = "The number of public subnets to create"
  type        = number
  default     = 2
}

```

### [outputs.tf](http://outputs.tf/)

```hcl
output "vpc_id" {
  description = "The ID of the VPC"
  value       = aws_vpc.main.id
}

output "public_subnet_ids" {
  description = "The IDs of the public subnets"
  value       = aws_subnet.public[*].id
}

```

### Usando o M√≥dulo

Para usar o m√≥dulo em uma configura√ß√£o principal, voc√™ pode referenci√°-lo da seguinte forma:

```hcl
module "vpc" {
  source              = "./modules/vpc"
  cidr_block          = "10.0.0.0/16"
  vpc_name            = "my-vpc"
  public_subnet_count = 2
}

```

### Pontos Importantes

- **source**: Especifica o caminho do m√≥dulo. Pode ser um caminho local, um reposit√≥rio Git, ou um registro de m√≥dulos.
- **Vari√°veis**: As vari√°veis definidas no m√≥dulo s√£o passadas como argumentos.
- **Sa√≠das**: As sa√≠das do m√≥dulo podem ser usadas na configura√ß√£o principal.

### Conclus√£o

Os m√≥dulos do Terraform s√£o uma poderosa ferramenta para organizar, reutilizar e padronizar a infraestrutura como c√≥digo. Eles ajudam a manter o c√≥digo limpo, modular e f√°cil de manter.

## Modulo Networking

- Crie um diret√≥rio chamado de **modules** e √© dentro desse diret√≥rio que ser√£o criados os nosso m√≥dulos

- Dentro do diret√≥rio modules, crie um diret√≥rio chamado de **network**

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20118.png)

- Para fazer a chamada do m√≥dulo √© necess√°rio que crie um arquivo chamado de `modules.tf`

```hcl
module "eks_network" {
    source = "./modules/network"
    # Passando as vari√°veis para o m√≥dulo
    project_name = var.project_name
    cidr_block = var.cidr_block
    tags = local.tags
}
```

./modules/network/variables.tf

```hcl
variable "cidr_block" {
  type        = string
  description = "Networking CIDR block to be used for the VPC"
}

variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type = map
  description = "Tags to be added to AWS resources"
}
```

Execute o seguinte comando:

```hcl
terraform init
terraform plan
terraform apply -auto-approve
```

## Output do M√≥dulo

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20119.png)

1

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.

output "subnet_pub_1a" {
  value = aws_subnet.eks_subnet_public_1a.id

}

output "subnet_pub_1b" {
  value = aws_subnet.eks_subnet_public_1b.id

}

output "subnet_priv_1a" {
  value = aws_subnet.eks_subnet_private_1a.id

}

output "subnet_priv_1b" {
  value = aws_subnet.eks_subnet_private_1b.id

}
```

2

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.
output "subnet_pub_1a" {
    value = module.eks_network.subnet_pub_1a
  
}
```

## M√≥dulo do Cluster

## EKS Cluster IAM role

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20120.png)

iam.tf

```hcl
resource "aws_iam_role" "eks_cluster_role" {
  name = "${var.project_name}-cluster-role"

  # Terraform's "jsonencode" function converts a
  # Terraform expression result to valid JSON syntax.
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Sid    = ""
        Principal = {
          Service = "eks.amazonaws.com"
        }
      },
    ]
  })

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-cluster-role"
    }
  )
}
```

variables.tf

```hcl
variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type        = map
  description = "Tags to be added to AWS resources"
}
```

modules.tf

```hcl
module "eks_network" {
  source = "./modules/network"
  # Passando as vari√°veis para o m√≥dulo
  project_name = var.project_name
  cidr_block   = var.cidr_block
  tags         = local.tags
}

module "eks_cluster" {
    source = "./modules/cluster"
    project_name = var.project_name
    tags = local.tags
}
```

```hcl
terraform init
terraform fmt -recursive
terraform plan
terraform apply -auto-approve
```

- Agora vamos colocar as permiss√µes (policy)
- https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy_attachment - iam policy attachment

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20121.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20122.png)

```hcl
# Arquivo respons√°vel por criar as permiss√µes necess√°rias para o EKS gerenciar clusters

# Vamos criar uma role para o EKS gerenciar clusters
resource "aws_iam_role" "eks_cluster_role" {
  name = "${var.project_name}-cluster-role"

  # Terraform's "jsonencode" function converts a
  # Terraform expression result to valid JSON syntax.
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Sid    = ""
        Principal = {
          Service = "eks.amazonaws.com"
        }
      },
    ]
  })

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-cluster-role"
    }
  )
}

# Vamos anexar a policy AmazonEKSClusterPolicy ao role criado que permite o EKS gerenciar clusters
resource "aws_iam_policy_attachment" "eks_cluster_role_attachment" {
  name       = "${var.project_name}-cluster-role-attachment"
  roles      = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}
```

# üìïOBSüìï: Toda vez que voc√™ cria uma m√≥dulo, necess√°rio que execute o seguinte comenado:

```hcl
terraform init
```

## Cria√ß√£o do cluster EKS

```hcl
resource "aws_eks_cluster" "eks_cluster" {

  name     = "${var.project_name}-cluster"
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      var.public_subnet_1a,
      var.public_subnet_1b
    ]
    endpoint_private_access = true
    endpoint_public_access  = true
  }

  # For√ßar a depend√™ncia para que o cluster s√≥ seja criado ap√≥s a role e a policy serem criadas
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_role_attachment
  ]

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-cluster"
    }
  )

}
```

Este c√≥digo Terraform cria um cluster EKS (Elastic Kubernetes Service) na AWS com as seguintes configura√ß√µes:

- **Nome do Cluster:** Utiliza a vari√°vel project_name concatenada com "-cluster"
- **IAM Role:** Associa a role criada anteriormente (eks_cluster_role) para permitir que o EKS gerencie recursos AWS
- **Configura√ß√£o de VPC:**
    - Utiliza duas subnets p√∫blicas (1a e 1b)
    - Permite acesso tanto privado quanto p√∫blico ao endpoint do cluster
- **Depend√™ncias:** Garante que a policy seja anexada √† role antes da cria√ß√£o do cluster atrav√©s do depends_on
- **Tags:** Mescla as tags padr√£o definidas com uma tag Name espec√≠fica para o cluster

Este √© um exemplo de infraestrutura como c√≥digo que cria um cluster EKS b√°sico e seguro na AWS, pronto para hospedar aplica√ß√µes Kubernetes.

Variables

```hcl
variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type        = map(any)
  description = "Tags to be added to AWS resources"
}

variable "public_subnet_1a" {
  type        = string
  description = "Subnet to create EKS Cluster AZ 1a"

}

variable "public_subnet_1b" {
  type        = string
  description = "Subnet to create EKS Cluster AZ 1b"

}
```

```hcl
module "eks_network" {
  source = "./modules/network"
  # Passando as vari√°veis para o m√≥dulo
  project_name = var.project_name
  cidr_block   = var.cidr_block
  tags         = local.tags
}

module "eks_cluster" {
  source           = "./modules/cluster"
  project_name     = var.project_name
  tags             = local.tags
  public_subnet_1a = module.eks_network.subnet_pub_1a
  public_subnet_1b = module.eks_network.subnet_pub_1b
}
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20123.png)

```hcl
terraform apply -auto-aprove
```

## Alterando a regra de HTTPS para API server

module/cluster

```hcl
output "eks_vpc_config" {
    value = aws_eks_cluster.eks_cluster.vpc_config
  
}
```

./projeto

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.
output "subnet_pub_1a" {
  value = module.eks_network.subnet_pub_1a

}

# Fazendo a chama do output do m√≥dulo cluster imprimir a configura√ß√£o da VPC
output "eks_vpc_config" {
    value = module.eks_cluster.eks_vpc_config
  
}
```

sg-rule.tf

```hcl
resource "aws_security_group_rule" "eks_cluster_sg_rule" {
  type              = "ingress"
  from_port         = 443
  to_port           = 443
  protocol          = "tcp"
  cidr_blocks       = [
    "0.0.0.0/0"
  ]
  security_group_id = aws_eks_cluster.eks_cluster.vpc_config[0].cluster_security_group_id

}
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20124.png)

üö®OBSüö® 

O [0] √© usado porque vpc_config retorna uma lista de configura√ß√µes VPC, mesmo que normalmente s√≥ exista uma configura√ß√£o. Em Terraform, quando um atributo retorna uma lista ou array, precisamos especificar qual elemento queremos acessar usando o √≠ndice.

Neste caso, [0] est√° acessando o primeiro (e √∫nico) elemento da lista de configura√ß√µes VPC do cluster EKS. √â uma particularidade da API do AWS EKS que requer este acesso indexado, mesmo quando sabemos que s√≥ existe uma configura√ß√£o VPC.

Se tent√°ssemos acessar cluster_security_group_id diretamente sem o [0], receber√≠amos um erro porque o Terraform n√£o saberia qual elemento da lista usar.

```hcl
terraform fmt -recursive
terraform plan 
terraform apply -auto-approve
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20125.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20126.png)

A imagem mostra a interface de gerenciamento de grupos de seguran√ßa (Security Groups) no console da AWS. Especificamente, est√° exibindo uma regra de entrada (inbound rule) para HTTPS na porta 443 que foi configurada para o cluster EKS (Elastic Kubernetes Service). A regra permite tr√°fego HTTPS de entrada (IPv4) de qualquer origem (0.0.0.0/0) para o grupo de seguran√ßa do cluster EKS.

Os elementos principais vis√≠veis na imagem s√£o:

- Um grupo de seguran√ßa associado ao cluster EKS (eks-cluster-sg-eksdevopment)
- Uma regra de entrada configurada para:
- Protocolo: TCP
- Porta: 443 (HTTPS)
- Origem: 0.0.0.0/0 (qualquer IP)
- Tipo: IPv4

Esta configura√ß√£o permite que a API do cluster Kubernetes seja acessada de forma segura atrav√©s de HTTPS de qualquer localiza√ß√£o na internet.

## Entendendo o Fingerprint do OIDC

O Fingerprint OIDC (OpenID Connect) √© uma sequ√™ncia √∫nica de caracteres que identifica e verifica a autenticidade de um provedor OIDC. No contexto do EKS (Elastic Kubernetes Service), o fingerprint √© usado para estabelecer uma rela√ß√£o de confian√ßa segura entre o cluster EKS e o provedor de identidade.

Principais aspectos do Fingerprint OIDC:

- **Fun√ß√£o de Seguran√ßa:** Garante que a comunica√ß√£o est√° sendo feita com o provedor OIDC leg√≠timo, prevenindo ataques de intermedi√°rio (man-in-the-middle)
- **Autentica√ß√£o IAM:** Permite que pods no EKS assumam roles do IAM atrav√©s da autentica√ß√£o OIDC, eliminando a necessidade de gerenciar credenciais AWS manualmente
- **Integra√ß√£o com AWS:** O EKS usa o fingerprint para configurar a integra√ß√£o com o AWS IAM, permitindo um controle de acesso mais granular aos recursos AWS

O fingerprint √© especialmente importante quando se configura a integra√ß√£o do EKS com servi√ßos AWS usando o IAM Roles for Service Accounts (IRSA), pois ele valida a autenticidade do provedor OIDC do cluster.

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20127.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20128.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20129.png)

Vai abrir essa tela, onde voc√™ poder extrair o fingerprint ou baixar o certificar e imprimir via command line

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20130.png)

Feito o download, execute o seguinte comando:

```bash
openssl x509 -in ~/Downloads/eks-us-east-1-amazonaws-com.pem -text 
```

```bash
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            08:92:fd:5c:66:da:c2:96:7a:22:c1:86:bc:3a:9d:c8
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: C=US, O=Amazon, CN=Amazon RSA 2048 M02
        Validity
            Not Before: Mar 28 00:00:00 2024 GMT
            Not After : Apr 26 23:59:59 2025 GMT
        Subject: CN=*.eks.us-east-1.amazonaws.com
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:b8:89:5a:60:c6:38:0e:98:c2:9d:0b:6b:58:4c:
                    76:bd:cb:3b:ae:2d:d6:02:8e:90:ba:c4:dd:55:80:
                    8e:f5:b9:38:92:c5:41:9f:e9:1d:cd:fd:17:64:e6:
                    f8:f3:d8:23:ae:dd:fb:15:80:c8:7a:70:2a:6a:ac:
                    f9:71:53:fe:6e:3f:4f:c3:15:f4:06:75:5c:41:6d:
                    9e:2f:61:10:bd:8c:86:e0:c8:a9:b0:a8:e1:09:30:
                    3f:fd:07:78:86:1f:3d:ca:a4:1a:67:5c:37:2e:ba:
                    5a:96:b0:2b:6e:ba:ac:63:9c:c2:cb:e2:1d:54:74:
                    e7:80:3c:dc:21:3b:24:31:28:c1:e5:f8:b7:eb:09:
                    97:82:b9:e8:4a:72:1f:97:33:ff:25:85:d2:a0:63:
                    22:e9:f2:8d:4d:9b:b2:2c:34:6d:5a:37:a8:8d:a1:
                    9e:ab:c4:4c:94:57:0e:9f:57:ea:59:0d:a2:2b:c6:
                    46:1f:ee:e9:d0:ac:07:d4:59:1d:76:da:95:c6:cd:
                    64:6d:bf:95:46:64:b8:eb:72:2f:1e:bf:86:69:80:
                    65:62:23:36:f6:2c:ea:93:16:6f:20:75:ee:9a:51:
                    19:40:9b:8b:8c:0c:5c:e6:67:f8:8e:a8:45:7e:e7:
                    6b:ce:f5:9d:04:e9:8a:39:33:77:49:66:a4:28:ee:
                    e5:29
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Authority Key Identifier: 
                C0:31:52:CD:5A:50:C3:82:7C:74:71:CE:CB:E9:9C:F9:7A:EB:82:E2
            X509v3 Subject Key Identifier: 
                84:6D:59:0C:A6:BB:EA:1B:4B:65:55:CF:F8:4D:CF:DC:69:D8:1E:31
            X509v3 Subject Alternative Name: 
                DNS:*.eks.us-east-1.amazonaws.com
            X509v3 Certificate Policies: 
                Policy: 2.23.140.1.2.1
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 CRL Distribution Points: 
                Full Name:
                  URI:http://crl.r2m02.amazontrust.com/r2m02.crl
            Authority Information Access: 
                OCSP - URI:http://ocsp.r2m02.amazontrust.com
                CA Issuers - URI:http://crt.r2m02.amazontrust.com/r2m02.cer
            X509v3 Basic Constraints: critical
                CA:FALSE
            CT Precertificate SCTs: 
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : CF:11:56:EE:D5:2E:7C:AF:F3:87:5B:D9:69:2E:9B:E9:
                                1A:71:67:4A:B0:17:EC:AC:01:D2:5B:77:CE:CC:3B:08
                    Timestamp : Mar 28 21:41:13.052 2024 GMT
                    Extensions: none
                    Signature : ecdsa-with-SHA256
                                30:45:02:20:72:37:F7:6F:A4:E4:3B:3B:EF:B9:CC:E0:
                                2C:0D:AF:EC:63:8C:2A:F8:F4:24:3F:BC:72:09:0D:92:
                                57:A2:43:3B:02:21:00:FB:E5:4F:0F:2C:8C:2E:28:CF:
                                CB:41:91:F1:90:48:BC:07:2B:61:B8:DF:25:74:C5:DC:
                                69:22:CD:E2:4B:54:92
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : 7D:59:1E:12:E1:78:2A:7B:1C:61:67:7C:5E:FD:F8:D0:
                                87:5C:14:A0:4E:95:9E:B9:03:2F:D9:0E:8C:2E:79:B8
                    Timestamp : Mar 28 21:41:13.054 2024 GMT
                    Extensions: none
                    Signature : ecdsa-with-SHA256
                                30:45:02:20:20:64:84:C6:9F:F6:07:51:DD:B0:9B:2F:
                                15:C5:6C:73:44:5D:48:7B:B7:73:FF:4B:16:94:28:37:
                                04:E9:49:88:02:21:00:DB:9C:D9:F5:A0:6D:FE:6A:E1:
                                0D:C3:1C:D7:9E:55:F2:A9:F0:69:D4:C4:AA:DB:4B:74:
                                FE:17:30:49:8C:B0:F0
                Signed Certificate Timestamp:
                    Version   : v1 (0x0)
                    Log ID    : E6:D2:31:63:40:77:8C:C1:10:41:06:D7:71:B9:CE:C1:
                                D2:40:F6:96:84:86:FB:BA:87:32:1D:FD:1E:37:8E:50
                    Timestamp : Mar 28 21:41:13.073 2024 GMT
                    Extensions: none
                    Signature : ecdsa-with-SHA256
                                30:44:02:20:51:45:E2:4A:6A:70:D8:07:17:34:62:D9:
                                C9:CB:E0:76:CD:02:96:65:9E:57:3E:45:EF:48:C9:85:
                                E2:0A:57:6A:02:20:64:65:48:2C:D7:68:AE:DA:E5:86:
                                53:DE:D5:50:02:02:B7:6F:B3:66:48:63:51:5D:E2:F8:
                                ED:E4:62:1D:72:AD
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        59:8e:4e:c5:1e:eb:5a:d6:05:4b:4f:71:01:86:08:1e:50:45:
        ef:07:54:c5:01:fa:d3:38:bf:22:a0:07:64:84:42:14:8d:71:
        1d:db:09:d0:15:2f:df:49:4b:ca:21:ad:56:be:03:85:e0:68:
        f3:5f:c8:12:b0:51:a5:79:92:91:da:03:24:0f:b9:77:f2:73:
        dc:41:6d:6e:b5:68:82:31:12:00:b0:20:05:f4:86:b8:95:f4:
        40:bb:21:8f:24:e5:f4:ed:ab:d3:4a:a4:c4:88:5b:2a:64:8f:
        4a:5f:49:41:5c:6a:b7:68:d3:68:1e:a9:0f:74:a3:59:7e:94:
        fa:5d:fd:ec:db:35:2f:aa:48:6f:5e:35:f2:85:68:ec:b7:87:
        c0:47:84:c7:a0:0f:30:8c:a3:ce:57:dc:5d:d0:87:03:a9:55:
        29:e8:8d:17:c4:87:f9:05:be:87:a6:4e:a4:65:c9:21:4e:a5:
        94:18:e0:58:08:6c:72:aa:c6:46:3d:40:3e:9b:8a:58:65:64:
        54:2c:1e:02:32:0e:70:4b:96:57:c1:6a:99:fa:75:87:d8:e6:
        79:49:06:e9:2a:01:89:fa:e5:e3:53:ef:84:ad:28:a9:0d:cb:
        94:01:7a:3a:9e:a7:ec:92:da:da:b9:57:91:fe:8b:89:e9:d9:
        0d:59:52:b9
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
```

OU

```bash
openssl x509 -in ~/Downloads/eks-us-east-1-amazonaws-com.pem -fingerprint -noout | cut -d = -f 2 | sed 's/://g' | tr '[:upper:]' '[:lower:]'

# Output
9451ad2b53c7f41fab22886cc07d482085336561

```

## Cria√ß√£o do OIDC Provider

```hcl
data "tls_certificate" "eks_oidc_tls_certificate" {
    url = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer
}

```

Output

module/cluster

```hcl
output "oidc" {
    value = aws_eks_cluster.eks_cluster.identity
  
}
```

Depois de debugar o problema, chegamos a essa configura√ß√£o

```hcl
output "eks_vpc_config" {
    value = aws_eks_cluster.eks_cluster.vpc_config
  
}

output "oidc" {
    value = data.tls_certificate.eks_oidc_tls_certificate.certificates[*].sha1_fingerprint
}
```

./project

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.
output "subnet_pub_1a" {
  value = module.eks_network.subnet_pub_1a

}

# Fazendo a chama do output do m√≥dulo cluster imprimir a configura√ß√£o da VPC
output "eks_vpc_config" {
    value = module.eks_cluster.eks_vpc_config
  
}

output "oidc" {
    value = module.eks_cluster.oidc
  
}
```

sg-rule.tf

```hcl
resource "aws_security_group_rule" "eks_cluster_sg_rule" {
  type              = "ingress"
  from_port         = 443
  to_port           = 443
  protocol          = "tcp"
  cidr_blocks       = [
    "0.0.0.0/0"
  ]
  security_group_id = aws_eks_cluster.eks_cluster.vpc_config[0].cluster_security_group_id

}
```

oidc.tf

```hcl
data "tls_certificate" "eks_oidc_tls_certificate" {
    url = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer
}

resource "aws_iam_openid_connect_provider" "eks_oidc" {
    client_id_list = [
        "sts.amazonaws.com"
    ]
    thumbprint_list = data.tls_certificate.eks_oidc_tls_certificate.certificates[*].sha1_fingerprint
    url = data.tls_certificate.eks_oidc_tls_certificate.url

    tags = merge(
        var.tags,
        {
            Name = "${var.project_name}-oidc"
        }
    )

  
}
```

```hcl
terraform init
terraform plan
terraform apply -auto-approve
```

## M√≥dulo do Managed Node Group

Um Managed Node Group √© um recurso do Amazon EKS (Elastic Kubernetes Service) que automatiza o provisionamento e gerenciamento dos n√≥s (nodes) do cluster Kubernetes.

Principais caracter√≠sticas do Managed Node Group:

- **Gerenciamento Automatizado:** A AWS gerencia automaticamente o ciclo de vida dos n√≥s, incluindo atualiza√ß√µes de sistema operacional, patches de seguran√ßa e atualiza√ß√µes do Kubernetes
- **Auto Scaling:** Suporta escalabilidade autom√°tica baseada em demanda, podendo aumentar ou diminuir o n√∫mero de n√≥s conforme necess√°rio
- **Integra√ß√£o com IAM:** Utiliza roles IAM para gerenciar permiss√µes e acessos de forma segura
- **Facilidade de Manuten√ß√£o:** Permite atualiza√ß√µes rolling dos n√≥s sem interrup√ß√£o do servi√ßo
- **Monitoramento:** Integra√ß√£o com CloudWatch para m√©tricas e logs dos n√≥s

O Managed Node Group √© especialmente √∫til porque:

- Reduz a complexidade operacional do gerenciamento de n√≥s
- Minimiza o trabalho manual de manuten√ß√£o e atualiza√ß√£o
- Garante alta disponibilidade e confiabilidade do cluster
- Oferece melhor custo-benef√≠cio por otimizar recursos automaticamente

## Configura√ß√£o do kubeconfig para kubectl

Comando para gerar um kubeconfig

```hcl
aws eks update-kubeconfig --region us-east-1 --name eksdevelopment-cluster
```

Variables

```hcl
variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type        = map(any)
  description = "Tags to be added to AWS resources"
}

variable "cluster_name" {
  type        = string
  description = "Name of the EKS cluster"
  
}

variable "subnet_private_1a" {
  type        = string
  description = "Subnet to create EKS Cluster AZ 1a"
  
}

variable "subnet_private_1b" {
  type        = string
  description = "Subnet to create EKS Cluster AZ 1b"
  
}

```

```hcl
resource "aws_eks_node_group" "eks" {
    cluster_name = var.cluster_name
    node_group_name = "${var.project_name}-nodegroup"
    node_role_arn = aws_iam_role.eks_mng_role.arn
    subnet_ids = [
        var.subnet_private_1a,
        var.subnet_private_1b,
    ]

    scaling_config {
        desired_size = 1
        max_size = 1
        min_size = 1
    }

    depends_on = [ 
        aws_iam_role_policy_attachment.eks_mng_role_attachment_worker,
        aws_iam_role_policy_attachment.eks_mng_role_attachment_ecr,
        aws_iam_role_policy_attachment.eks_mng_role_attachment_cni
     ]

}
```

Este √© um recurso do Terraform que define um grupo de n√≥s gerenciados (Managed Node Group) para um cluster EKS. Vamos analisar seus principais componentes:

- **Identifica√ß√£o e Localiza√ß√£o**: O grupo de n√≥s √© vinculado a um cluster EKS espec√≠fico e ser√° implantado nas subnets privadas especificadas (subnet_private_1a e 1b)
- **Configura√ß√£o de Escala**: Define os par√¢metros de escalabilidade do grupo, onde neste caso est√° configurado para manter exatamente 1 n√≥ (desired_size = 1, max_size = 1, min_size = 1)
- **Depend√™ncias**: O recurso depende de tr√™s pol√≠ticas IAM essenciais:
    - Uma pol√≠tica para workers do EKS
    - Uma pol√≠tica para acesso ao ECR (Container Registry)
    - Uma pol√≠tica para o CNI (Container Network Interface)

Este recurso √© parte fundamental do EKS pois √© respons√°vel por gerenciar os n√≥s onde os containers Kubernetes ser√£o executados. Ele oferece benef√≠cios importantes como:

- Gerenciamento automatizado do ciclo de vida dos n√≥s
- Capacidade de auto-scaling baseado em demanda
- Integra√ß√£o com IAM para gerenciamento seguro de permiss√µes

```hcl
module "eks_network" {
  source = "./modules/network"
  # Passando as vari√°veis para o m√≥dulo
  project_name = var.project_name
  cidr_block   = var.cidr_block
  tags         = local.tags
}

module "eks_cluster" {
  source           = "./modules/cluster"
  project_name     = var.project_name
  tags             = local.tags
  public_subnet_1a = module.eks_network.subnet_pub_1a
  public_subnet_1b = module.eks_network.subnet_pub_1b
}

module "eks_managed_node_group" {
  source       = "./modules/managed-node-group"
  project_name = var.project_name
  cluster_name = module.eks_cluster.cluster_name
  subnet_private_1a = module.eks_network.subnet_priv_1a
  subnet_private_1b = module.eks_network.subnet_priv_1b
  tags         = local.tags

}
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20131.png)

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20132.png)

## M√≥dulo do AWS Load Balancer Controller

## IAM policy do Controller

Segue o link para pegar o json iam_policy

https://docs.aws.amazon.com/eks/latest/userguide/lbc-helm.html

```bash
wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json
```

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "iam:AWSServiceName": "elasticloadbalancing.amazonaws.com"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeAccountAttributes",
                "ec2:DescribeAddresses",
                "ec2:DescribeAvailabilityZones",
                "ec2:DescribeInternetGateways",
                "ec2:DescribeVpcs",
                "ec2:DescribeVpcPeeringConnections",
                "ec2:DescribeSubnets",
                "ec2:DescribeSecurityGroups",
                "ec2:DescribeInstances",
                "ec2:DescribeNetworkInterfaces",
                "ec2:DescribeTags",
                "ec2:GetCoipPoolUsage",
                "ec2:DescribeCoipPools",
                "ec2:GetSecurityGroupsForVpc",
                "elasticloadbalancing:DescribeLoadBalancers",
                "elasticloadbalancing:DescribeLoadBalancerAttributes",
                "elasticloadbalancing:DescribeListeners",
                "elasticloadbalancing:DescribeListenerCertificates",
                "elasticloadbalancing:DescribeSSLPolicies",
                "elasticloadbalancing:DescribeRules",
                "elasticloadbalancing:DescribeTargetGroups",
                "elasticloadbalancing:DescribeTargetGroupAttributes",
                "elasticloadbalancing:DescribeTargetHealth",
                "elasticloadbalancing:DescribeTags",
                "elasticloadbalancing:DescribeTrustStores",
                "elasticloadbalancing:DescribeListenerAttributes",
                "elasticloadbalancing:DescribeCapacityReservation"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "cognito-idp:DescribeUserPoolClient",
                "acm:ListCertificates",
                "acm:DescribeCertificate",
                "iam:ListServerCertificates",
                "iam:GetServerCertificate",
                "waf-regional:GetWebACL",
                "waf-regional:GetWebACLForResource",
                "waf-regional:AssociateWebACL",
                "waf-regional:DisassociateWebACL",
                "wafv2:GetWebACL",
                "wafv2:GetWebACLForResource",
                "wafv2:AssociateWebACL",
                "wafv2:DisassociateWebACL",
                "shield:GetSubscriptionState",
                "shield:DescribeProtection",
                "shield:CreateProtection",
                "shield:DeleteProtection"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AuthorizeSecurityGroupIngress",
                "ec2:RevokeSecurityGroupIngress"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateSecurityGroup"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags"
            ],
            "Resource": "arn:aws:ec2:*:*:security-group/*",
            "Condition": {
                "StringEquals": {
                    "ec2:CreateAction": "CreateSecurityGroup"
                },
                "Null": {
                    "aws:RequestTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags",
                "ec2:DeleteTags"
            ],
            "Resource": "arn:aws:ec2:*:*:security-group/*",
            "Condition": {
                "Null": {
                    "aws:RequestTag/elbv2.k8s.aws/cluster": "true",
                    "aws:ResourceTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "ec2:AuthorizeSecurityGroupIngress",
                "ec2:RevokeSecurityGroupIngress",
                "ec2:DeleteSecurityGroup"
            ],
            "Resource": "*",
            "Condition": {
                "Null": {
                    "aws:ResourceTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:CreateLoadBalancer",
                "elasticloadbalancing:CreateTargetGroup"
            ],
            "Resource": "*",
            "Condition": {
                "Null": {
                    "aws:RequestTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:CreateListener",
                "elasticloadbalancing:DeleteListener",
                "elasticloadbalancing:CreateRule",
                "elasticloadbalancing:DeleteRule"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:AddTags",
                "elasticloadbalancing:RemoveTags"
            ],
            "Resource": [
                "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
                "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
                "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
            ],
            "Condition": {
                "Null": {
                    "aws:RequestTag/elbv2.k8s.aws/cluster": "true",
                    "aws:ResourceTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:AddTags",
                "elasticloadbalancing:RemoveTags"
            ],
            "Resource": [
                "arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*",
                "arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*",
                "arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*",
                "arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:ModifyLoadBalancerAttributes",
                "elasticloadbalancing:SetIpAddressType",
                "elasticloadbalancing:SetSecurityGroups",
                "elasticloadbalancing:SetSubnets",
                "elasticloadbalancing:DeleteLoadBalancer",
                "elasticloadbalancing:ModifyTargetGroup",
                "elasticloadbalancing:ModifyTargetGroupAttributes",
                "elasticloadbalancing:DeleteTargetGroup",
                "elasticloadbalancing:ModifyListenerAttributes",
                "elasticloadbalancing:ModifyCapacityReservation"
            ],
            "Resource": "*",
            "Condition": {
                "Null": {
                    "aws:ResourceTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:AddTags"
            ],
            "Resource": [
                "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
                "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
                "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
            ],
            "Condition": {
                "StringEquals": {
                    "elasticloadbalancing:CreateAction": [
                        "CreateTargetGroup",
                        "CreateLoadBalancer"
                    ]
                },
                "Null": {
                    "aws:RequestTag/elbv2.k8s.aws/cluster": "false"
                }
            }
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:RegisterTargets",
                "elasticloadbalancing:DeregisterTargets"
            ],
            "Resource": "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "elasticloadbalancing:SetWebAcl",
                "elasticloadbalancing:ModifyListener",
                "elasticloadbalancing:AddListenerCertificates",
                "elasticloadbalancing:RemoveListenerCertificates",
                "elasticloadbalancing:ModifyRule"
            ],
            "Resource": "*"
        }
    ]
}

```

```hcl
variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type        = map(any)
  description = "Tags to be added to AWS resources"
}

```

```hcl
resource "aws_iam_policy" "policy" {

  name = "${var.project_name}-policy"

  policy = file("${path.module}/policy.json")
  tags   = var.tags

}
```

Este resource do Terraform cria uma pol√≠tica IAM (Identity and Access Management) na AWS com as seguintes caracter√≠sticas:

- **Tipo do Recurso:** "aws_iam_policy" - Indica que estamos criando uma pol√≠tica IAM na AWS
- **Nome do Recurso:** O nome da pol√≠tica √© din√¢mico, usando a vari√°vel "project_name" concatenada com "-policy"
- **Conte√∫do da Pol√≠tica:** O conte√∫do √© carregado de um arquivo JSON local usando a fun√ß√£o file(). O caminho "${path.module}/policy.json" referencia um arquivo policy.json no mesmo diret√≥rio do m√≥dulo
- **Tags:** As tags s√£o atribu√≠das atrav√©s da vari√°vel "var.tags", permitindo a marca√ß√£o do recurso para melhor organiza√ß√£o

Este resource √© comumente usado para definir permiss√µes e controles de acesso na AWS, sendo parte fundamental da infraestrutura como c√≥digo com Terraform.

```hcl
module "eks_network" {
  source = "./modules/network"
  # Passando as vari√°veis para o m√≥dulo
  project_name = var.project_name
  cidr_block   = var.cidr_block
  tags         = local.tags
}

module "eks_cluster" {
  source           = "./modules/cluster"
  project_name     = var.project_name
  tags             = local.tags
  public_subnet_1a = module.eks_network.subnet_pub_1a
  public_subnet_1b = module.eks_network.subnet_pub_1b
}

module "eks_managed_node_group" {
  source            = "./modules/managed-node-group"
  project_name      = var.project_name
  cluster_name      = module.eks_cluster.cluster_name
  subnet_private_1a = module.eks_network.subnet_priv_1a
  subnet_private_1b = module.eks_network.subnet_priv_1b
  tags              = local.tags

}

module "eks_load_balancer_controller" {
  source       = "./modules/aws-load-balancer-controller"
  project_name = var.project_name
  tags         = local.tags

}
```

```hcl
terraform init
terraform fmt -recursive
terraform plan
terraform apply -auto-approve
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20133.png)

## IAM role do Controller

```hcl
data "aws_caller_identity" "current" {}

data "aws_region" "current" {}
```

```hcl
# Arquivo respons√°vel por criar as permiss√µes necess√°rias para o EKS gerenciar clusters

# Vamos criar uma role para o EKS gerenciar clusters
resource "aws_iam_role" "eks_controller_role" {
  name = "${var.project_name}-aws-load-balancer-controller"

  assume_role_policy = <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/oidc.eks.${data.aws_region.current.name}.amazonaws.com/id/${local.oidc}"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "oidc.eks.${data.aws_region.current.name}.amazonaws.com/id/${local.oidc}:aud": "sts.amazonaws.com",
                    "oidc.eks.${data.aws_region.current.name}.amazonaws.com/id/${local.oidc}:sub": "system:serviceaccount:kube-system:aws-load-balancer-controller"
                }
            }
        }
    ]
}
EOF

  tags = merge(
    var.tags,
    {
      Name = "${var.project_name}-aws-load-balancer-controller"
    }
  )
}

resource "aws_iam_role_policy_attachment" "eks_controller_role_attachment" {
  role       = aws_iam_role.eks_controller_role.name
  policy_arn = aws_iam_policy.eks_load_balancer_controller.arn
}
# Vamos anexar a policy AmazonEKSClusterPolicy ao role criado que permite o EKS gerenciar clusters
# resource "aws_iam_role_policy_attachment" "eks_controller_role_attachment" {
#   role       = aws_iam_role.eks_controller_role.name
#   policy_arn = aws_iam_policy.eks_load_balancer_controller.arn
# }

# data - vai buscar informa√ß√µes do ambiente atual da AWS para utilizar no c√≥digo
```

```hcl
variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type        = map(any)
  description = "Tags to be added to AWS resources"
}

variable "oidc" {
  type        = string
  description = "HTTPS URL from OIDC provider of the EKS cluster"

}
```

```hcl
output "eks_vpc_config" {
  value = aws_eks_cluster.eks_cluster.vpc_config

}

# output "oidc" {
#   value = data.tls_certificate.eks_oidc_tls_certificate.certificates[*].sha1_fingerprint
# }

output "cluster_name" {
  value = aws_eks_cluster.eks_cluster.id
}

output "oidc" {
  value = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer
}
```

```hcl
module "eks_network" {
  source = "./modules/network"
  # Passando as vari√°veis para o m√≥dulo
  project_name = var.project_name
  cidr_block   = var.cidr_block
  tags         = local.tags
}

module "eks_cluster" {
  source           = "./modules/cluster"
  project_name     = var.project_name
  tags             = local.tags
  public_subnet_1a = module.eks_network.subnet_pub_1a
  public_subnet_1b = module.eks_network.subnet_pub_1b
}

module "eks_managed_node_group" {
  source            = "./modules/managed-node-group"
  project_name      = var.project_name
  cluster_name      = module.eks_cluster.cluster_name
  subnet_private_1a = module.eks_network.subnet_priv_1a
  subnet_private_1b = module.eks_network.subnet_priv_1b
  tags              = local.tags

}

module "eks_load_balancer_controller" {
  source       = "./modules/aws-load-balancer-controller"
  project_name = var.project_name
  tags         = local.tags
  oidc         = module.eks_cluster.oidc

}
```

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.
output "subnet_pub_1a" {
  value = module.eks_network.subnet_pub_1a

}

# Fazendo a chama do output do m√≥dulo cluster imprimir a configura√ß√£o da VPC
output "eks_vpc_config" {
  value = module.eks_cluster.eks_vpc_config

}

# output "oidc" {
#   value = module.eks_cluster.oidc

# }

output "oidc" {
  value = module.eks_cluster.oidc

}
```

Agora abra o terraform e use a fun√ß√£o split() para cortar a url do oidc gerada pelo output configurado

```hcl
terraform console
```

```hcl
> split("/", "https://oidc.eks.us-east-1.amazonaws.com/id/BFD8257863CB12694B306F4C4E96F33B")
tolist([
  "https:",
  "",
  "oidc.eks.us-east-1.amazonaws.com",
  "id",
  "BFD8257863CB12694B306F4C4E96F33B",
])
> split("/", "https://oidc.eks.us-east-1.amazonaws.com/id/BFD8257863CB12694B306F4C4E96F33B")[4]
"BFD8257863CB12694B306F4C4E96F33B"
>  
```

local.tf

```hcl
locals {
  oidc = split("/", var.oidc)[4]
}
```

```hcl
terraform plan
terraform apply -auto-approve
```

## Provider do Kubernetes

O Provider do Kubernetes no Terraform √© um plugin que permite gerenciar recursos do Kubernetes atrav√©s do Terraform. Ele possibilita a cria√ß√£o e gerenciamento de recursos como deployments, services, namespaces e outros objetos do Kubernetes de forma declarativa.

Configura√ß√£o b√°sica do provider:

```hcl
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}
```

Principais caracter√≠sticas:

- **Integra√ß√£o com diferentes clusters:** Pode ser configurado para trabalhar com qualquer cluster Kubernetes, seja local ou na nuvem
- **Autentica√ß√£o flex√≠vel:** Suporta diferentes m√©todos de autentica√ß√£o, incluindo tokens, certificados e kubeconfig
- **Recursos nativos:** Permite gerenciar todos os tipos de recursos nativos do Kubernetes
- **Custom Resources:** Suporte para recursos personalizados (CRDs) do Kubernetes

Exemplo de cria√ß√£o de um namespace:

```hcl
resource "kubernetes_namespace" "example" {
  metadata {
    name = "my-namespace"
  }
}
```

O provider do Kubernetes √© especialmente √∫til quando voc√™ precisa:

- Gerenciar infraestrutura e aplica√ß√µes Kubernetes em um √∫nico fluxo de trabalho
- Manter consist√™ncia entre diferentes ambientes
- Automatizar o processo de deployment de aplica√ß√µes
- Integrar com outros recursos gerenciados pelo Terraform

C√≥digo usado na aula:

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.81.0"
    }
    kubernetes = {
      source = "hashicorp/kubernetes"
      version = "2.35.1"
    }
  }
  backend "s3" {
    bucket = "comunidadedevops-eks-s3"
    key    = "dev/terraform.tfstate"
    region = "us-east-1"
  }

}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}

provider "kubernetes" {
  host                   = module.eks_cluster.endpoint
  cluster_ca_certificate = base64decode(module.eks_cluster.certificate_authority)
  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    args        = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
    command     = "aws"
  }
}
```

module cluster

```hcl
# Output do cluster

# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.

# Imprimir a configura√ß√£o da VPC
output "eks_vpc_config" {
  value = aws_eks_cluster.eks_cluster.vpc_config

}

# output "oidc" {
#   value = data.tls_certificate.eks_oidc_tls_certificate.certificates[*].sha1_fingerprint
# }

# Imprimir o nome do cluster
output "cluster_name" {
  value = aws_eks_cluster.eks_cluster.id
}

# Impress√£o do OIDC
output "oidc" {
  value = aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer
}

# Imprimir o certificado da autoridade
output "certificate_authority" {
  value = aws_eks_cluster.eks_cluster.certificate_authority[0].data
  
}

output "endpoint" {
  value = aws_eks_cluster.eks_cluster.endpoint
  
}
```

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.
output "subnet_pub_1a" {
  value = module.eks_network.subnet_pub_1a

}

# Fazendo a chama do output do m√≥dulo cluster imprimir a configura√ß√£o da VPC
output "eks_vpc_config" {
  value = module.eks_cluster.eks_vpc_config

}

# output "oidc" {
#   value = module.eks_cluster.oidc

# }

output "oidc" {
  value = module.eks_cluster.oidc

}

output "ca" {
  value = module.eks_cluster.certificate_authority
  
}

output "endpoint" {
  value = module.eks_cluster.endpoint
  
}
```

## Deploy da Service Account

V√° na documenta√ß√£o do Kubernetes em providers no terraform e pesquise por Service Account

```hcl
https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/resources/service_account
```

```hcl
resource "kubernetes_service_account" "eks_controller_sa" {
  metadata {
    name = "aws-load-balancer-controller"
    namespace = "kube-system"
    annotations = {
        "eks.amazonaws.com/role-arn" = aws_iam_role.eks_controller_role.arn
    }
  }
}

```

```hcl
terraform plan
terraform apply -auto-approve
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20134.png)

## Deploy do helm chart

- Segue a documenta√ß√£o para configurar o Helm Providar

https://registry.terraform.io/providers/hashicorp/helm/latest

```hcl
helm repo add eks https://aws.github.io/eks-charts
helm repo update eks

helm search repo aws-load-balancer-controller
```

```hcl
resource "aws_iam_policy" "eks_load_balancer_controller" {

  name = "${var.project_name}-aws-load-balancer-controller"

  policy = file("${path.module}/iam_policy.json")
  tags   = var.tags

}
```

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.81.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "2.35.1"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "2.17.0"
    }
  }
  backend "s3" {
    bucket = "comunidadedevops-eks-s3"
    key    = "dev/terraform.tfstate"
    region = "us-east-1"
  }

}

provider "aws" {
  # Configuration options
  region = "us-east-1"
}

provider "kubernetes" {
  host                   = module.eks_cluster.endpoint
  cluster_ca_certificate = base64decode(module.eks_cluster.certificate_authority)
  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    args        = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
    command     = "aws"
  }
}

provider "helm" {
  kubernetes {
    host                   = module.eks_cluster.endpoint
    cluster_ca_certificate = base64decode(module.eks_cluster.certificate_authority)
    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      args        = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
      command     = "aws"
    }
  }
}
```

Este bloco de configura√ß√£o do Terraform define a estrutura b√°sica do projeto com tr√™s componentes principais:

### 1. Configura√ß√£o do Terraform

- Define os provedores necess√°rios (AWS, Kubernetes e Helm) com suas vers√µes espec√≠ficas
- Configura o backend S3 para armazenar o estado do Terraform remotamente no bucket "comunidadedevops-eks-s3"

### 2. Provider AWS

- Configura o provedor AWS para a regi√£o us-east-1

### 3. Provider Kubernetes

- Configura a conex√£o com o cluster EKS usando:
- Endpoint do cluster
- Certificado de autoridade do cluster
- Token de autentica√ß√£o obtido via comando AWS CLI

### 4. Provider Helm

- Configura o Helm para interagir com o cluster Kubernetes
- Usa as mesmas configura√ß√µes de autentica√ß√£o do provider Kubernetes
- Permite o gerenciamento de releases Helm via Terraform

module network

```hcl
# Output √© utilizado para exportar valores de um m√≥dulo para outros m√≥dulos ou para o m√≥dulo raiz.

output "subnet_pub_1a" {
  value = aws_subnet.eks_subnet_public_1a.id

}

output "subnet_pub_1b" {
  value = aws_subnet.eks_subnet_public_1b.id

}

output "subnet_priv_1a" {
  value = aws_subnet.eks_subnet_private_1a.id

}

output "subnet_priv_1b" {
  value = aws_subnet.eks_subnet_private_1b.id

}

output "vpc_id" {
  value = aws_vpc.eks_vpc.id
}
```

```hcl
variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "tags" {
  type        = map(any)
  description = "Tags to be added to AWS resources"
}

variable "oidc" {
  type        = string
  description = "HTTPS URL from OIDC provider of the EKS cluster"

}

variable "cluster_name" {
  type        = string
  description = "EKS cluster name"
  
}

variable "vpc_id" {
  type        = string
  description = "EKS cluster name"
}
```

```hcl
resource "helm_release" "eks_helm_controller" {
    name       = "aws-load-balancer-controller"
    repository = "https://aws.github.io/eks-charts"
    chart      = "aws-load-balancer-controller"
    version    = "1.11.0"
    namespace  = "kube-system"

    #timeout = 700

    set {
    name  = "clusterName"
    value = var.cluster_name
    }

    set {
    name  = "serviceAccount.create"
    value = "false"
    }

    set {
    name  = "serviceAccount.name"
    value = "aws-load-balancer-controller"
    }
    set {
    name  = "vpcId"
    value = var.vpc_id
    }
}
```

Este recurso do Terraform configura a instala√ß√£o do AWS Load Balancer Controller usando o Helm:

- **name:** Define o nome da release do Helm como "aws-load-balancer-controller"
- **repository:** Especifica o reposit√≥rio Helm oficial da AWS para o EKS
- **chart:** Nome do chart Helm a ser instalado
- **version:** Vers√£o espec√≠fica do chart (1.11.0)
- **namespace:** Instala no namespace kube-system

As configura√ß√µes do 'set' definem valores importantes:

- **clusterName:** Nome do cluster EKS onde ser√° instalado
- **serviceAccount.create:** Definido como false pois a Service Account j√° foi criada separadamente
- **serviceAccount.name:** Nome da Service Account existente
- **vpcId:** ID da VPC onde o controller ir√° operar

```hcl
module "eks_network" {
  source = "./modules/network"
  # Passando as vari√°veis para o m√≥dulo
  project_name = var.project_name
  cidr_block   = var.cidr_block
  tags         = local.tags
}

module "eks_cluster" {
  source           = "./modules/cluster"
  project_name     = var.project_name
  tags             = local.tags
  public_subnet_1a = module.eks_network.subnet_pub_1a
  public_subnet_1b = module.eks_network.subnet_pub_1b
}

module "eks_managed_node_group" {
  source            = "./modules/managed-node-group"
  project_name      = var.project_name
  cluster_name      = module.eks_cluster.cluster_name
  subnet_private_1a = module.eks_network.subnet_priv_1a
  subnet_private_1b = module.eks_network.subnet_priv_1b
  tags              = local.tags

}

module "eks_load_balancer_controller" {
  source       = "./modules/aws-load-balancer-controller"
  project_name = var.project_name
  tags         = local.tags
  oidc         = module.eks_cluster.oidc
  cluster_name = module.eks_cluster.cluster_name
  vpc_id       = module.eks_network.vpc_id 

}
```

```hcl
terraform init
terraform fmt -recursive
terraform plan
terraform apply -auto-approve
```

Tava tendo um problema ao aplicar o helm controller mas achei uma solu√ß√£o para isso.

- O erro era: Context dealine exceeded

```bash
‚îÇ Warning: Helm release "aws-load-balancer-controller" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again.

‚îÇ

‚îÇ   with module.eks_aws_load_balancer_controller.helm_release.eks_helm_controller,

‚îÇ   on modules/aws-load-balancer-controller/helm.tf line 1, in resource "helm_release" "eks_helm_controller":

‚îÇ    1: resource "helm_release" "eks_helm_controller" {

‚îÇ Error: context deadline exceeded

‚îÇ

‚îÇ   with module.eks_aws_load_balancer_controller.helm_release.eks_helm_controller,

‚îÇ   on modules/aws-load-balancer-controller/helm.tf line 1, in resource "helm_release" "eks_helm_controller":

‚îÇ    1: resource "helm_release" "eks_helm_controller" {

‚îÇ

```

- O script acima j√° est√£o corrigidos j√°, para que n√£o possa ter nenhum equ√≠voco

- Solu√ß√£o:

```hcl
inseri isso no helm.tf:

     set {
        name  = "vpcId"
        value = var.vpc_id
      }

Inseri essa linha no output.tf que est√° dentro da pasta network:

    output "vpc_id" {
      value = aws_vpc.eks_vpc.id
    }

inseri isso no variables.tf que est√° dentro da pasta aws-load-balancer-controller

    variable "vpc_id" {
      type        = string
      description = "EKS cluster name"
    }

E no modules.tf que est√° na raiz do projeto onde tem essa parte:

    module "eks_aws_load_balancer_controller" {
      source       = "./modules/aws-load-balancer-controller"
      project_name = var.project_name
      tags         = local.tags
      oidc         = module.eks_cluster.oidc
      cluster_name = module.eks_cluster.cluster_name
      vpc_id       = module.eks_network.vpc_id 
    }

Inseri a linha :
vpc_id       = module.eks_network.vpc_id

```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20135.png)

## Super m√≥dulo

## Instala√ß√£o do terraform-docs

Link da documenta√ß√£o

https://terraform-docs.io/user-guide/installation/

Depois de instalado, execute o seguinte comando para verfificar se realmente o terraform docs foi instalado

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20136.png)

## Gerando a documenta√ß√£o do m√≥dulo e de sub-m√≥dulos

Comandos

```bash
# Vai documentar em json
terraform-docs json .

terraform-docs markdown . --output-file README.md 
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20137.png)

## Pre-commit com terraform-docs

```yaml
repos:
- repo: https://github.com/antonbabenko/pre-commit-terraform
  rev: v1.96.3 # Get the latest from: https://github.com/antonbabenko/pre-commit-terraform/releases
  hooks:
    - id: terraform_fmt
      args:
      - --args=-recursive
    - id: terraform_validate
    - id: terraform_docs
      args:
        - --args=--output-file README.md
```

Toda a altera√ß√£o que ocorrer, ele ir√° documentar

## Modularizando o super m√≥dulo

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.81.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "2.35.1"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "2.17.0"
    }
  }
  

}

provider "aws" {
  # Configuration options
  region = var.region
}

provider "kubernetes" {
  host                   = module.eks_cluster.endpoint
  cluster_ca_certificate = base64decode(module.eks_cluster.certificate_authority)
  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    args        = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
    command     = "aws"
  }
}

provider "helm" {
  kubernetes {
    host                   = module.eks_cluster.endpoint
    cluster_ca_certificate = base64decode(module.eks_cluster.certificate_authority)
    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      args        = ["eks", "get-token", "--cluster-name", module.eks_cluster.cluster_name]
      command     = "aws"
    }
  }
}
```

```hcl
variable "cidr_block" {
  type        = string
  description = "Networking CIDR block to be used for the VPC"
}

variable "project_name" {
  type        = string
  description = "Project name to be used to name the resources (Name tag)"

}

variable "region" {
  type        = string
  description = "AWS region to deploy the resources"

}

variable "tags" {
  type        = map(any)
  description = "Tags to be added to all AWS resources"

}

```

Depois delete o [locals.tf](http://locals.tf) da raiz

## Utilizando o m√≥dulo

https://developer.hashicorp.com/terraform/language/modules/sources#github

- V√° no github e pegue a url de seu projeto criado e importe ele como modulo

```hcl
module "eks" {
  source = "git@github.com:Erick-Fernandes-dev/terraform-eks.git?ref=main"
}
```

![image.png](Terraform%201472e2f65d7e80cda081d84211707a20/image%20138.png)

```hcl
locals {
  tags = {
    Department   = "DevOps"
    Organization = "Infrastructure and Operations"
    Project      = "EKS"
    Environment  = "Development"
  }
}
```

```hcl
module "eks" {
  source = "git@github.com:Erick-Fernandes-dev/terraform-eks.git?ref=main"
  cidr_block = "10.0.0.0/16"
  project_name = "demoeks"
  region = "us-east-1"
  tags = local.tags
}
```

```hcl
terraform {
  backend "s3" {
    bucket = "comunidadedevops-eks-s3"
    key    = "qa/terraform.tfstate"
    region = "us-east-1"
  }
}

```

- Vamos baixar esse m√≥dulo

```hcl
terraform init
terraform plan
terrafrom fmt -recursive
terraform apply -auto-approve
```